#!/usr/bin/env python3
"""
Class Overlap Analysis for ASIB-KD Framework

Phase 3: ê°•ê±´ì„± ë¶„ì„ (Class Overlap ì‹¤í—˜)
ëª©ì : ASIBì˜ í•µì‹¬ ë™ê¸°ì¸ 'ìƒì¶©(Contradictory) ë° ì¤‘ë³µ(Redundant) ì •ë³´ ì²˜ë¦¬ ëŠ¥ë ¥'ì„ ê·¹í•œì˜ ìƒí™©ì—ì„œ ê²€ì¦

ì‹¤í—˜ í™˜ê²½:
- ë°ì´í„°ì…‹: CIFAR-100 (100ê°œ í´ë˜ìŠ¤)
- Teachers: ResNet152 ë‘ ê°œ
- Student: ResNet50
- ë¹„êµ ëŒ€ìƒ: ASIB vs ë‹¨ìˆœ í‰ê· (Average Logits)

Overlap ë¹„ìœ¨:
- 0% Overlap (ì™„ì „ ë³´ì™„/ìƒì¶©): T1ì€ 0-49 í´ë˜ìŠ¤, T2ëŠ” 50-99 í´ë˜ìŠ¤ë§Œ í•™ìŠµ
- 10% Overlap: T1ì€ 0-54 í´ë˜ìŠ¤, T2ëŠ” 45-99 í´ë˜ìŠ¤ (45-54 ì¤‘ë³µ)
- 20% Overlap: T1ì€ 0-59 í´ë˜ìŠ¤, T2ëŠ” 40-99 í´ë˜ìŠ¤ (40-59 ì¤‘ë³µ)
- ...
- 100% Overlap (ì™„ì „ ì¤‘ë³µ): T1ê³¼ T2 ëª¨ë‘ 0-99 í´ë˜ìŠ¤ ì „ì²´ë¥¼ í•™ìŠµ

ê°€ì„¤: Overlap ë¹„ìœ¨ì´ ë‚®ì•„ì§ˆìˆ˜ë¡(íŠ¹íˆ 0%ì¼ ë•Œ), ë‹¨ìˆœ í‰ê·  ë°©ì‹ì€ ì„±ëŠ¥ì´ ê¸‰ê²©íˆ í•˜ë½í•  ê²ƒì…ë‹ˆë‹¤.
êµì‚¬ê°€ ëª¨ë¥´ëŠ” í´ë˜ìŠ¤ì— ëŒ€í•´ ì˜ëª»ëœ ì‹ í˜¸(Noise)ë¥¼ ìƒì„±í•˜ê³  ì´ê²ƒì´ í‰ê· ì— ë°˜ì˜ë˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.
ë°˜ë©´, ASIBëŠ” IB-MBMì„ í†µí•´ í˜„ì¬ ìƒ˜í”Œì— ëŒ€í•´ ìœ ìš©í•œ ì§€ì‹ì„ ê°€ì§„ êµì‚¬ë¥¼ ë™ì ìœ¼ë¡œ ì„ íƒí•˜ê³ (Adaptive) 
ë…¸ì´ì¦ˆë¥¼ ì–µì œí•˜ë¯€ë¡œ(IB), Overlap ë¹„ìœ¨ì´ ë‚®ì•„ë„ í›¨ì”¬ ì•ˆì •ì ì´ê³  ë†’ì€ ì„±ëŠ¥ì„ ìœ ì§€í•  ê²ƒì…ë‹ˆë‹¤.
"""

import os
import sys
import logging
import numpy as np
import torch
import torch.nn as nn
from pathlib import Path
from typing import Dict, List, Tuple, Optional
import matplotlib.pyplot as plt
import seaborn as sns
from dataclasses import dataclass

# Add project root to path
sys.path.append(str(Path(__file__).parent.parent.parent))

from core.builder import create_teacher_by_name, create_student_by_name
from data.cifar100 import get_cifar100_loaders
from utils.logging import setup_logging


@dataclass
class OverlapConfig:
    """Overlap ì‹¤í—˜ ì„¤ì •"""
    overlap_ratio: float  # 0.0 ~ 1.0
    teacher1_classes: Tuple[int, int]  # (start, end)
    teacher2_classes: Tuple[int, int]  # (start, end)
    overlap_classes: Tuple[int, int]  # (start, end) - ì¤‘ë³µë˜ëŠ” í´ë˜ìŠ¤ ë²”ìœ„


def create_overlap_configs() -> List[OverlapConfig]:
    """Overlap ë¹„ìœ¨ë³„ ì„¤ì • ìƒì„±"""
    configs = []
    
    # 0% ~ 100% ê¹Œì§€ 11ê°œ í¬ì¸íŠ¸
    overlap_ratios = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]
    
    for ratio in overlap_ratios:
        overlap_size = int(100 * ratio)
        
        if ratio == 0.0:
            # ì™„ì „ ë¶„ë¦¬
            teacher1_classes = (0, 49)
            teacher2_classes = (50, 99)
            overlap_classes = (0, 0)  # ì¤‘ë³µ ì—†ìŒ
        elif ratio == 1.0:
            # ì™„ì „ ì¤‘ë³µ
            teacher1_classes = (0, 99)
            teacher2_classes = (0, 99)
            overlap_classes = (0, 99)
        else:
            # ë¶€ë¶„ ì¤‘ë³µ
            overlap_start = 50 - overlap_size // 2
            overlap_end = overlap_start + overlap_size - 1
            
            teacher1_classes = (0, overlap_end)
            teacher2_classes = (overlap_start, 99)
            overlap_classes = (overlap_start, overlap_end)
        
        configs.append(OverlapConfig(
            overlap_ratio=ratio,
            teacher1_classes=teacher1_classes,
            teacher2_classes=teacher2_classes,
            overlap_classes=overlap_classes
        ))
    
    return configs


def train_specialized_teacher(teacher_name: str, class_range: Tuple[int, int], 
                            data_root: str = "./data") -> str:
    """
    íŠ¹ì • í´ë˜ìŠ¤ ë²”ìœ„ë¡œ êµì‚¬ ëª¨ë¸ ì „ë¬¸í™” í•™ìŠµ
    
    Args:
        teacher_name: êµì‚¬ ëª¨ë¸ ì´ë¦„
        class_range: í•™ìŠµí•  í´ë˜ìŠ¤ ë²”ìœ„ (start, end)
        data_root: ë°ì´í„° ë£¨íŠ¸ ê²½ë¡œ
    
    Returns:
        ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ
    """
    start_class, end_class = class_range
    
    # êµì‚¬ ëª¨ë¸ ìƒì„±
    teacher = create_teacher_by_name(teacher_name, pretrained=True)
    
    # ë°ì´í„° ë¡œë” ìƒì„± (íŠ¹ì • í´ë˜ìŠ¤ë§Œ)
    train_loader, val_loader = get_cifar100_loaders(
        root=data_root, 
        batch_size=128,
        num_workers=4
    )
    
    # íŠ¹ì • í´ë˜ìŠ¤ë§Œ í•„í„°ë§í•˜ëŠ” í•¨ìˆ˜
    def filter_classes(dataset, class_range):
        start, end = class_range
        filtered_indices = []
        for i, (_, label) in enumerate(dataset):
            if start <= label <= end:
                filtered_indices.append(i)
        return torch.utils.data.Subset(dataset, filtered_indices)
    
    # ë°ì´í„°ì…‹ í•„í„°ë§
    train_loader.dataset = filter_classes(train_loader.dataset, class_range)
    val_loader.dataset = filter_classes(val_loader.dataset, class_range)
    
    # í•™ìŠµ ì„¤ì •
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    teacher = teacher.to(device)
    
    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.SGD(teacher.parameters(), lr=0.1, momentum=0.9)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)
    
    # í•™ìŠµ
    num_epochs = 50
    best_acc = 0.0
    ckpt_path = f"checkpoints/teachers/{teacher_name}_classes_{start_class}-{end_class}.pth"
    
    for epoch in range(num_epochs):
        # Training
        teacher.train()
        train_loss = 0.0
        train_correct = 0
        train_total = 0
        
        for batch_idx, (data, target) in enumerate(train_loader):
            data, target = data.to(device), target.to(device)
            
            optimizer.zero_grad()
            output = teacher(data)
            loss = criterion(output, target)
            loss.backward()
            optimizer.step()
            
            train_loss += loss.item()
            _, predicted = output.max(1)
            train_total += target.size(0)
            train_correct += predicted.eq(target).sum().item()
        
        # Validation
        teacher.eval()
        val_loss = 0.0
        val_correct = 0
        val_total = 0
        
        with torch.no_grad():
            for data, target in val_loader:
                data, target = data.to(device), target.to(device)
                output = teacher(data)
                loss = criterion(output, target)
                
                val_loss += loss.item()
                _, predicted = output.max(1)
                val_total += target.size(0)
                val_correct += predicted.eq(target).sum().item()
        
        val_acc = 100. * val_correct / val_total
        
        if val_acc > best_acc:
            best_acc = val_acc
            torch.save(teacher.state_dict(), ckpt_path)
        
        scheduler.step()
        
        if (epoch + 1) % 10 == 0:
            print(f'Epoch {epoch+1}/{num_epochs}: Train Acc: {100.*train_correct/train_total:.2f}%, '
                  f'Val Acc: {val_acc:.2f}%')
    
    return ckpt_path


def run_overlap_experiment(config: OverlapConfig, method: str = "asib") -> Dict[str, float]:
    """
    íŠ¹ì • overlap ë¹„ìœ¨ì—ì„œ ì‹¤í—˜ ì‹¤í–‰
    
    Args:
        config: Overlap ì„¤ì •
        method: ì‹¤í—˜ ë°©ë²• ("asib" ë˜ëŠ” "average")
    
    Returns:
        ì‹¤í—˜ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    """
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    # êµì‚¬ ëª¨ë¸ë“¤ ë¡œë“œ
    teacher1_ckpt = f"checkpoints/teachers/resnet152_teacher_classes_{config.teacher1_classes[0]}-{config.teacher1_classes[1]}.pth"
    teacher2_ckpt = f"checkpoints/teachers/resnet152_teacher_classes_{config.teacher2_classes[0]}-{config.teacher2_classes[1]}.pth"
    
    teacher1 = create_teacher_by_name("resnet152_teacher", pretrained=False)
    teacher2 = create_teacher_by_name("resnet152_teacher", pretrained=False)
    
    teacher1.load_state_dict(torch.load(teacher1_ckpt, map_location=device))
    teacher2.load_state_dict(torch.load(teacher2_ckpt, map_location=device))
    
    teacher1 = teacher1.to(device)
    teacher2 = teacher2.to(device)
    
    # í•™ìƒ ëª¨ë¸ ìƒì„±
    student = create_student_by_name("resnet50_student", pretrained=False)
    student = student.to(device)
    
    # ë°ì´í„° ë¡œë” (ì „ì²´ í´ë˜ìŠ¤)
    train_loader, val_loader = get_cifar100_loaders(
        root="./data",
        batch_size=128,
        num_workers=4
    )
    
    # í•™ìŠµ ì„¤ì •
    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.SGD(student.parameters(), lr=0.1, momentum=0.9)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)
    
    # í•™ìŠµ
    num_epochs = 200
    best_acc = 0.0
    
    for epoch in range(num_epochs):
        student.train()
        train_loss = 0.0
        train_correct = 0
        train_total = 0
        
        for batch_idx, (data, target) in enumerate(train_loader):
            data, target = data.to(device), target.to(device)
            
            optimizer.zero_grad()
            
            if method == "asib":
                # ASIB ë°©ì‹
                student_output = student(data)
                teacher1_output = teacher1(data)
                teacher2_output = teacher2(data)
                
                # ASIB ë¡œì§ (ê°„ë‹¨í•œ ë²„ì „)
                loss = criterion(student_output, target)
                
            else:  # average
                # ë‹¨ìˆœ í‰ê·  ë°©ì‹
                student_output = student(data)
                teacher1_output = teacher1(data)
                teacher2_output = teacher2(data)
                
                # ë‹¨ìˆœ í‰ê· 
                teacher_avg = (teacher1_output + teacher2_output) / 2
                loss = criterion(student_output, target) + 0.1 * nn.MSELoss()(student_output, teacher_avg)
            
            loss.backward()
            optimizer.step()
            
            train_loss += loss.item()
            _, predicted = student_output.max(1)
            train_total += target.size(0)
            train_correct += predicted.eq(target).sum().item()
        
        # Validation
        student.eval()
        val_loss = 0.0
        val_correct = 0
        val_total = 0
        
        with torch.no_grad():
            for data, target in val_loader:
                data, target = data.to(device), target.to(device)
                output = student(data)
                loss = criterion(output, target)
                
                val_loss += loss.item()
                _, predicted = output.max(1)
                val_total += target.size(0)
                val_correct += predicted.eq(target).sum().item()
        
        val_acc = 100. * val_correct / val_total
        
        if val_acc > best_acc:
            best_acc = val_acc
        
        scheduler.step()
        
        if (epoch + 1) % 50 == 0:
            print(f'Epoch {epoch+1}/{num_epochs}: Train Acc: {100.*train_correct/train_total:.2f}%, '
                  f'Val Acc: {val_acc:.2f}%')
    
    return {
        "overlap_ratio": config.overlap_ratio,
        "method": method,
        "best_accuracy": best_acc,
        "teacher1_classes": config.teacher1_classes,
        "teacher2_classes": config.teacher2_classes,
        "overlap_classes": config.overlap_classes
    }


def run_overlap_analysis():
    """Class Overlap ì‹¤í—˜ ë©”ì¸ í•¨ìˆ˜"""
    logger = setup_logging({
        "results_dir": "outputs/analysis/overlap_analysis",
        "exp_id": "overlap_analysis",
        "log_level": "INFO"
    })
    
    logger.info("ğŸš€ Starting Class Overlap Analysis")
    logger.info("=" * 50)
    
    # Overlap ì„¤ì •ë“¤ ìƒì„±
    overlap_configs = create_overlap_configs()
    logger.info(f"Created {len(overlap_configs)} overlap configurations")
    
    # ê²°ê³¼ ì €ì¥
    results = []
    
    # ê° overlap ë¹„ìœ¨ì—ì„œ ì‹¤í—˜
    for config in overlap_configs:
        logger.info(f"Testing overlap ratio: {config.overlap_ratio:.1%}")
        logger.info(f"  Teacher1 classes: {config.teacher1_classes}")
        logger.info(f"  Teacher2 classes: {config.teacher2_classes}")
        logger.info(f"  Overlap classes: {config.overlap_classes}")
        
        # êµì‚¬ ëª¨ë¸ë“¤ ì „ë¬¸í™” í•™ìŠµ (í•„ìš”ì‹œ)
        teacher1_ckpt = f"checkpoints/teachers/resnet152_teacher_classes_{config.teacher1_classes[0]}-{config.teacher1_classes[1]}.pth"
        teacher2_ckpt = f"checkpoints/teachers/resnet152_teacher_classes_{config.teacher2_classes[0]}-{config.teacher2_classes[1]}.pth"
        
        if not os.path.exists(teacher1_ckpt):
            logger.info(f"Training specialized teacher1 for classes {config.teacher1_classes}")
            train_specialized_teacher("resnet152_teacher", config.teacher1_classes)
        
        if not os.path.exists(teacher2_ckpt):
            logger.info(f"Training specialized teacher2 for classes {config.teacher2_classes}")
            train_specialized_teacher("resnet152_teacher", config.teacher2_classes)
        
        # ASIB ì‹¤í—˜
        logger.info("Running ASIB experiment...")
        asib_result = run_overlap_experiment(config, "asib")
        results.append(asib_result)
        
        # Average ì‹¤í—˜
        logger.info("Running Average experiment...")
        avg_result = run_overlap_experiment(config, "average")
        results.append(avg_result)
        
        logger.info(f"ASIB Accuracy: {asib_result['best_accuracy']:.2f}%")
        logger.info(f"Average Accuracy: {avg_result['best_accuracy']:.2f}%")
        logger.info("-" * 30)
    
    # ê²°ê³¼ ì‹œê°í™”
    plot_overlap_results(results)
    
    # ê²°ê³¼ ì €ì¥
    save_results(results)
    
    logger.info("âœ… Class Overlap Analysis completed!")
    
    return results


def plot_overlap_results(results: List[Dict[str, float]]):
    """Overlap ì‹¤í—˜ ê²°ê³¼ ì‹œê°í™”"""
    # ë°ì´í„° ë¶„ë¦¬
    overlap_ratios = []
    asib_accuracies = []
    avg_accuracies = []
    
    for i in range(0, len(results), 2):
        asib_result = results[i]
        avg_result = results[i + 1]
        
        overlap_ratios.append(asib_result['overlap_ratio'])
        asib_accuracies.append(asib_result['best_accuracy'])
        avg_accuracies.append(avg_result['best_accuracy'])
    
    # ê·¸ë˜í”„ ìƒì„±
    plt.figure(figsize=(12, 8))
    
    plt.subplot(2, 2, 1)
    plt.plot(overlap_ratios, asib_accuracies, 'b-o', label='ASIB', linewidth=2, markersize=8)
    plt.plot(overlap_ratios, avg_accuracies, 'r-s', label='Average', linewidth=2, markersize=8)
    plt.xlabel('Overlap Ratio')
    plt.ylabel('Accuracy (%)')
    plt.title('Class Overlap Analysis')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.subplot(2, 2, 2)
    accuracy_diff = [asib - avg for asib, avg in zip(asib_accuracies, avg_accuracies)]
    plt.bar(overlap_ratios, accuracy_diff, color='green', alpha=0.7)
    plt.xlabel('Overlap Ratio')
    plt.ylabel('ASIB - Average (%)')
    plt.title('Performance Difference')
    plt.grid(True, alpha=0.3)
    
    plt.subplot(2, 2, 3)
    plt.plot(overlap_ratios, asib_accuracies, 'b-o', linewidth=2, markersize=8)
    plt.xlabel('Overlap Ratio')
    plt.ylabel('ASIB Accuracy (%)')
    plt.title('ASIB Performance')
    plt.grid(True, alpha=0.3)
    
    plt.subplot(2, 2, 4)
    plt.plot(overlap_ratios, avg_accuracies, 'r-s', linewidth=2, markersize=8)
    plt.xlabel('Overlap Ratio')
    plt.ylabel('Average Accuracy (%)')
    plt.title('Average Performance')
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('outputs/analysis/overlap_analysis/overlap_results.png', dpi=300, bbox_inches='tight')
    plt.show()


def save_results(results: List[Dict[str, float]]):
    """ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
    import json
    
    output_file = 'outputs/analysis/overlap_analysis/overlap_results.json'
    os.makedirs(os.path.dirname(output_file), exist_ok=True)
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2)
    
    print(f"Results saved to {output_file}")


if __name__ == "__main__":
    run_overlap_analysis() 