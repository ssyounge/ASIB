# configs/minimal.yaml

batch_size: 128              # ↑ 가능하면 256 (GPU 여유 시)
num_workers: 2
disable_tqdm: true
dataset_root: "./data"
results_dir: "results"
device: "cuda"
seed: 42
checkpoint_dir: "checkpoints"

student_type: "convnext_tiny"

# ─ IB‑KD 하이퍼 ────────────────────────────────────
mbm_type: "VIB"
# ─ Latent 용량 ↑ ─
z_dim: 512

# ─ Student Proj MLP ─
proj_hidden_dim: 1024   # NEW
proj_use_bn: true

beta_bottleneck: 0.003       # IB KL 가중치
kd_alpha_init: 0.6          # KD 초반 가중치
kd_alpha_final: 0.05      # hard‑KD 비중 ↑
kd_T_init: 5                # 초기 온도
kd_T_final: 1.0           # 더 hard logit
kd_warmup_frac: 0.05         # 앞부분 스케줄 고정 비율
kd_schedule_granularity: "step"
kd_sched_pow: 1.0         # 선형 스케줄
ce_alpha: 1.0                # 🔸 CE 그대로
latent_alpha: 0.8         # feature 정렬 가중 ↑
label_smoothing: 0.05         # 🔹 CE 안정화
randaug_N: 2                 # 🔹 RandAug(N,M)
randaug_M: 7
 
teacher_weight_decay: 5e-4
student_weight_decay: 5e-4
teacher_lr: 1e-3        # MBM Adam LR ↑
student_lr: 7e-4
lr_schedule: "cosine"
grad_clip_norm: 1.0

# run evaluation after training loop by default
eval_after_train: true

# ─ Data Mixing ───────────────────────────────────
mixup_alpha: 0.4             # CutMix 대신 MixUp 경량 사용
cutmix_alpha_distill: 0.0    # (0 = MixUp 우선)
 
# ─ 학습 스테이지 ──────────────────────────────────

teacher_iters: 20       # MBM 더 학습
student_iters: 60       # 조기 수렴

# ─ EMA 평가 ─────────────
use_ema: true
ema_decay: 0.992

# --- Teacher fine‑tune ---
finetune_epochs: 20
finetune_lr: 5e-4
finetune_weight_decay: 1e-5
teacher2_freeze_scope: "none"
