# configs/minimal.yaml

batch_size: 128              # ↑ 가능하면 256 (GPU 여유 시)
num_workers: 2           # 0이면 persistent 의미가 없어집니다
persistent_workers: true   # 데이터 로더 워커 유지 여부
disable_tqdm: true
dataset_root: "./data"
results_dir: "results"
device: "cuda"
seed: 42
checkpoint_dir: "checkpoints"
use_amp: false                 # mixed precision training
amp_dtype: "float16"           # AMP data type
log_kl: false                  # log KL divergence each step
train_mode: standard          # {standard|continual}
n_tasks: 10                    # train_mode=continual 일 때만 사용
buffer_size: 20_per_class     # (선택) μ‑prototype replay
teacher1_ckpt: "checkpoints/resnet152_ft.pth"
teacher2_ckpt: "checkpoints/efficientnet_b2_ft.pth"
student_ce_ckpt: "student_ce_best.pth"
finetune_partial_freeze: false
num_classes: 100        # 고정 클래스 수

student_type: "convnext_tiny"
# Conv stem stride
patch_stride2: false       # true 로 바꾸면 stride 2

# ─ Distillation method 선택 ──────────────────────
# method: vib        # {vib | dkd | crd | vanilla | ce}

# --- AT --------------------------
# method: at
alpha: 1000          # ← 기존 1 에서 ↑
layer_key: feat_4d_layer3

# --- FitNet ----------------------
method: fitnet
alpha_hint: 50       # ← 기존 1 에서 ↑
alpha_ce: 1

# ─ CRD 전용 ─
crd_alpha: 0.5
crd_T: 0.07

# ─ DKD 전용 ─
dkd_alpha: 1.0
dkd_beta: 8.0
dkd_T: 4.0
dkd_warmup: 5

# ─ Vanilla-KD 전용 ─
vanilla_alpha: 0.5
vanilla_T: 4.0

# ─ IB‑KD 하이퍼 ────────────────────────────────────
mbm_type: "GATE"
# ─ Latent 용량 ↑ ─
z_dim: 512

# ─ Student Proj MLP ─
proj_hidden_dim: 1024   # NEW
proj_use_bn: true

# ─ KD 스케줄 ① (더 hard·더 빨리) ────────────────

beta_bottleneck: 0.001       # IB KL 가중치
kd_alpha_init: 0.3
kd_alpha_final: 0.6         # 후반부 soft‑KD ⭢ hard‑KD
kd_T_init: 6                # 초기 온도
kd_T_final: 2.5           # 너무 hard logit ↓ generalization 저하 방지
kd_warmup_frac: 0.03      # warm-up 줄임
kd_schedule_granularity: "step"
kd_sched_pow: 1.0         # 선형
ce_alpha: 1.0                # 🔸 CE 그대로
# ─ Latent 정렬 ② ────────────────────────────────
latent_alpha: 0.8         # feature 정렬 가중 ↑
latent_mse_weight: 0.7
latent_angle_weight: 0.3
label_smoothing: 0.05         # 🔹 CE 안정화
randaug_N: 2                 # 🔹 RandAug(N,M)
randaug_M: 7
randaug_default_N: 2         # fallback RandAug(N,M) N
randaug_default_M: 9         # fallback RandAug(N,M) M
 
teacher_weight_decay: 5e-4
student_weight_decay: 5e-4
teacher_lr: 1e-3        # MBM Adam LR ↑
teacher_dropout_p: 0.3      # teacher classifier dropout
gate_dropout: 0.1       # GateMBM dropout 확정
student_lr: 1.0e-3          # convnext_tiny 는 1e‑3 이상에서 더 잘 수렴
lr_schedule: "cosine"
# 학습률 warm‑up
lr_warmup_epochs: 5        # 필요 시 숫자 조정
grad_clip_norm: 1.0
grad_scaler_init_scale: 1024  # AMP GradScaler initial scale

# run evaluation after training loop by default
eval_after_train: true

# ─ Data Mixing ───────────────────────────────────
mixup_alpha: 0.2
cutmix_alpha_distill: 0.5
 
# ─ 학습 스테이지 ──────────────────────────────────

teacher_iters: 20       # MBM 더 학습
student_iters: 90       # 정규화로 안정 ⇒ epoch 확장
student_warmup_epochs: 3    # cosine warm-up length for student training
min_lr_ratio_student: 0.05  # cosine scheduler minimum LR ratio

# ─ EMA 평가 ─────────────
use_ema: true
ema_decay: 0.992
ema_warmup_iters: 5
ema_initial_decay: 0.90

# --- Teacher fine‑tune ---
finetune_epochs: 30          # +10 epoch
finetune_lr: 1e-3            # LR ↑
finetune_weight_decay: 3e-4  # WD ↓

# ─ Teacher fine‑tune scheduler ─
finetune_warmup: 5           # NEW – warm‑up epoch
finetune_sched: "cosine"     # linear‑warmup + cosine
min_lr_ratio_finetune: 0.1   # cosine scheduler minimum LR ratio

# ─ Fine‑tune aug ─
finetune_randaug_N: 2
finetune_randaug_M: 7
finetune_mixup_alpha: 0.2

finetune_label_smoothing: 0.05
teacher2_freeze_scope: "none"

# --- feature distillation ---
# ConvNeXt tiny 기준 예시
feat_layers:  [2, 3]        # ConvNeXt 마지막 2 stage (더 추상적)
feat_weights: [0.5, 0.5]
feat_loss_weight: 0.2       # γ ↓  (정규화 후 0.2 ~ 0.3 권장; or [γ1,γ2,γ3])
