# configs/minimal.yaml

batch_size: 128              # â†‘ ê°€ëŠ¥í•˜ë©´ 256 (GPU ì—¬ìœ  ì‹œ)
num_workers: 2           # 0ì´ë©´ persistent ì˜ë¯¸ê°€ ì—†ì–´ì§‘ë‹ˆë‹¤
persistent_workers: true   # ë°ì´í„° ë¡œë” ì›Œì»¤ ìœ ì§€ ì—¬ë¶€
disable_tqdm: true
dataset_root: "./data"
results_dir: "results"
device: "cuda"
seed: 42
checkpoint_dir: "checkpoints"
use_amp: false                 # mixed precision training
amp_dtype: "float16"           # AMP data type
log_kl: false                  # log KL divergence each step
teacher1_ckpt: "checkpoints/resnet152_ft.pth"
teacher2_ckpt: "checkpoints/efficientnet_b2_ft.pth"
student_ce_ckpt: "student_ce_best.pth"
finetune_partial_freeze: false
num_classes: 100        # ê³ ì • í´ë˜ìŠ¤ ìˆ˜

student_type: "convnext_tiny"
# Conv stem stride
patch_stride2: false       # true ë¡œ ë°”ê¾¸ë©´ stride 2

# â”€ Distillation method ì„ íƒ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
method: vib        # {vib | dkd | crd | vanilla | ce}

# â”€ CRD ì „ìš© â”€
crd_alpha: 0.5
crd_T: 0.07

# â”€ DKD ì „ìš© â”€
dkd_alpha: 1.0
dkd_beta: 8.0
dkd_T: 4.0
dkd_warmup: 5

# â”€ Vanilla-KD ì „ìš© â”€
vanilla_alpha: 0.5
vanilla_T: 4.0

# â”€ IBâ€‘KD í•˜ì´í¼ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
mbm_type: "GATE"
# â”€ Latent ìš©ëŸ‰ â†‘ â”€
z_dim: 512

# â”€ Student Proj MLP â”€
proj_hidden_dim: 1024   # NEW
proj_use_bn: true

# â”€ KD ìŠ¤ì¼€ì¤„Â â‘ Â (ë” hardÂ·ë” ë¹¨ë¦¬) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

beta_bottleneck: 0.001       # IBÂ KLÂ ê°€ì¤‘ì¹˜
kd_alpha_init: 0.3
kd_alpha_final: 0.6         # í›„ë°˜ë¶€ softâ€‘KDÂ â­¢Â hardâ€‘KD
kd_T_init: 6                # ì´ˆê¸° ì˜¨ë„
kd_T_final: 2.5           # ë„ˆë¬´ hardÂ logitÂ â†“Â generalization ì €í•˜ ë°©ì§€
kd_warmup_frac: 0.03      # warm-up ì¤„ì„
kd_schedule_granularity: "step"
kd_sched_pow: 1.0         # ì„ í˜•
ce_alpha: 1.0                # ğŸ”¸ CEÂ ê·¸ëŒ€ë¡œ
# â”€ Latent ì •ë ¬Â â‘¡ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
latent_alpha: 0.8         # feature ì •ë ¬ ê°€ì¤‘ â†‘
latent_mse_weight: 0.7
latent_angle_weight: 0.3
label_smoothing: 0.05         # ğŸ”¹ CEÂ ì•ˆì •í™”
randaug_N: 2                 # ğŸ”¹ RandAug(N,M)
randaug_M: 7
randaug_default_N: 2         # fallback RandAug(N,M) N
randaug_default_M: 9         # fallback RandAug(N,M) M
 
teacher_weight_decay: 5e-4
student_weight_decay: 5e-4
teacher_lr: 1e-3        # MBM Adam LR â†‘
teacher_dropout_p: 0.3      # teacher classifier dropout
gate_dropout: 0.1       # GateMBM dropout í™•ì •
student_lr: 1.0e-3          # convnext_tiny ëŠ” 1eâ€‘3Â ì´ìƒì—ì„œ ë” ì˜ ìˆ˜ë ´
lr_schedule: "cosine"
# í•™ìŠµë¥  warmâ€‘up
lr_warmup_epochs: 5        # í•„ìš” ì‹œ ìˆ«ì ì¡°ì •
grad_clip_norm: 1.0
grad_scaler_init_scale: 1024  # AMP GradScaler initial scale

# run evaluation after training loop by default
eval_after_train: true

# â”€ DataÂ Mixing â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
mixup_alpha: 0.2
cutmix_alpha_distill: 0.5
 
# â”€ í•™ìŠµ ìŠ¤í…Œì´ì§€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

teacher_iters: 20       # MBM ë” í•™ìŠµ
student_iters: 90       # ì •ê·œí™”ë¡œ ì•ˆì • â‡’ epochÂ í™•ì¥
student_warmup_epochs: 3    # cosine warm-up length for student training
min_lr_ratio_student: 0.05  # cosine scheduler minimum LR ratio

# â”€ EMA í‰ê°€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
use_ema: true
ema_decay: 0.992
ema_warmup_iters: 5
ema_initial_decay: 0.90

# --- Teacher fineâ€‘tune ---
finetune_epochs: 30          # +10â€¯epoch
finetune_lr: 1e-3            # LR â†‘
finetune_weight_decay: 3e-4  # WD â†“

# â”€ Teacher fineâ€‘tune scheduler â”€
finetune_warmup: 5           # NEW â€“ warmâ€‘up epoch
finetune_sched: "cosine"     # linearâ€‘warmup + cosine
min_lr_ratio_finetune: 0.1   # cosine scheduler minimum LR ratio

# â”€ Fineâ€‘tune aug â”€
finetune_randaug_N: 2
finetune_randaug_M: 7
finetune_mixup_alpha: 0.2

finetune_label_smoothing: 0.05
teacher2_freeze_scope: "none"

# --- feature distillation ---
# ConvNeXt tiny ê¸°ì¤€ ì˜ˆì‹œ
feat_layers:  [2, 3]        # ConvNeXt ë§ˆì§€ë§‰ 2â€¯stage (ë” ì¶”ìƒì )
feat_weights: [0.5, 0.5]
feat_loss_weight: 0.2       # Î³ â†“  (ì •ê·œí™” í›„ 0.2Â ~Â 0.3 ê¶Œì¥)
