# configs/default.yaml

device: "cuda"          # "cuda" or "cpu"
seed: 42
dataset_name: "cifar100"
small_input: true
data_root: "./data"
batch_size: 128

teacher1_type: "resnet101"
teacher2_type: "efficientnet_b2"

teacher1_pretrained: true
teacher2_pretrained: true

teacher_lr: 0.0001           # 통합 Teacher 학습률
teacher_weight_decay: 0.0003
teacher_adapt_epochs: 5      # Teacher Adaptive Update epochs
mbm_lr_factor: 5.0           # MBM/Head LR 배수

synergy_ce_alpha: 0.3
teacher_adapt_alpha_kd: 0.2  # Teacher adaptive 시 KL 비중

reg_lambda: 1e-5   # Teacher param reg

student_type: "efficientnet_adapter"
student_lr: 0.01
student_weight_decay: 0.0005
student_epochs_per_stage: 15  # Student distill epochs per stage

ce_alpha: 0.5     # CE 비중
kd_alpha: 0.5     # KL 비중
temperature: 4.0

# --- Feature-KD 옵션 (추가) -------------------------
use_feat_kd: false        # true → MSE 활성화
feat_kd_alpha: 0.25       # λ_feat   (논문 α_feat)
feat_kd_key: "feat_2d"    # 어느 딕셔너리 key를 쓸지 (feat_4d 도 가능)
feat_kd_norm: "none"      # "none" | "l2"  (예: 벡터 정규화 후 MSE)

# disagreement-based sample weighting
use_disagree_weight: false
disagree_mode: "pred"       # "pred" | "both_wrong"
disagree_lambda_high: 1.0
disagree_lambda_low: 1.0

num_stages: 2

teacher_iters: 10
student_iters: 20

use_partial_freeze: true

mbm_in_dim: 3456
mbm_hidden_dim: 1024
mbm_out_dim: 2048
mbm_dropout: 0.0
synergy_head_dropout: 0.0
mbm_use_4d: false
mbm_attn_heads: 0

cutmix_alpha: 1.0
mixup_alpha: 0.0
label_smoothing: 0.0

log_filename: "train.log"
save_checkpoint: true
ckpt_dir: "./checkpoints"

# Optional teacher fine-tuning before ASMB stages
finetune_epochs: 0           # >0 => run fine-tuning step
finetune_lr: 0.001
finetune_weight_decay: 0.0005
finetune_use_cutmix: true
finetune_alpha: 1.0
finetune_ckpt1: "./checkpoints/teacher1_finetuned.pth"
finetune_ckpt2: "./checkpoints/teacher2_finetuned.pth"
