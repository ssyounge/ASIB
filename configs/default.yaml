# configs/default.yaml

device: "cuda"          # "cuda" or "cpu"
seed: 42
deterministic: true
dataset_name: "cifar100"
small_input: true
data_root: "./data"
batch_size: 128
use_amp: false
amp_dtype: float16
grad_scaler_init_scale: 1024


teacher1_type: "resnet101"
teacher2_type: "efficientnet_b2"

teacher1_pretrained: true
teacher2_pretrained: true
teacher1_use_adapter: false
teacher1_bn_head_only: false
teacher2_use_adapter: false
teacher2_bn_head_only: false

teacher_lr: 0.0001           # 통합 Teacher 학습률
teacher_weight_decay: 0.0003
teacher_adapt_epochs: 5      # Teacher Adaptive Update epochs
mbm_lr_factor: 5.0           # MBM/Head LR 배수

synergy_ce_alpha: 0.3
teacher_adapt_alpha_kd: 0.2  # Teacher adaptive 시 KL 비중

reg_lambda: 1e-5          # Teacher L2 regularisation
# --------------------------------------------------
# L2 regularisation for MBM + Synergy-Head (App. C)
#   - Setting it to 0.0 keeps current behaviour.
#   - Tune around 1e-5 ~ 1e-4 for best results.
mbm_reg_lambda: 0.0
# --------------------------------------------------

student_type: "efficientnet_adapter"
student_lr: 0.01
student_weight_decay: 0.0005
student_epochs_per_stage: 15  # Student distill epochs per stage

ce_alpha: 0.5     # CE 비중
kd_alpha: 0.5     # KL 비중
# ----- KD temperature (τ) schedule -----
temperature_schedule: "fixed"   # fixed | linear | cosine
tau_start: 4.0                  # 초기 τ
tau_end:   1.0                  # 최종 τ
tau_decay_epochs: 40            # 몇 epoch 동안 감소할지 (넘어가면 tau_end 고정)

# --- Feature-KD 옵션 (추가) -------------------------
use_feat_kd: false        # true → MSE 활성화
feat_kd_alpha: 0.25       # λ_feat   (논문 α_feat)
feat_kd_key: "feat_2d"    # 어느 딕셔너리 key를 쓸지 (feat_4d 도 가능)
feat_kd_norm: "none"      # "none" | "l2"  (예: 벡터 정규화 후 MSE)

# disagreement-based sample weighting
use_disagree_weight: false
disagree_mode: "pred"       # "pred" | "both_wrong"
disagree_lambda_high: 1.0
disagree_lambda_low: 1.0

num_stages: 2

teacher_iters: 10
student_iters: 20

use_partial_freeze: true

mbm_in_dim: 3456
mbm_hidden_dim: 1024
mbm_out_dim: 2048
mbm_dropout: 0.0
synergy_head_dropout: 0.0
mbm_use_4d: false
mbm_attn_heads: 0
mbm_type: "LA"        # "MLP" for old behaviour
mbm_r: 4
mbm_n_head: 1
mbm_learnable_q: false

cutmix_alpha: 1.0
cutmix_alpha_distill: 0.0
mixup_alpha: 0.0
label_smoothing: 0.0

log_filename: "train.log"
save_checkpoint: true
ckpt_dir: "./checkpoints"

# Optional teacher fine-tuning before ASMB stages
finetune_epochs: 0           # >0 => run fine-tuning step
finetune_lr: 0.001
finetune_weight_decay: 0.0005
finetune_use_cutmix: true
finetune_alpha: 1.0
finetune_ckpt1: "./checkpoints/teacher1_finetuned.pth"
finetune_ckpt2: "./checkpoints/teacher2_finetuned.pth"
