# configs/default.yaml

device: "cuda"
seed: 42
deterministic: true
dataset_name: "cifar100"
small_input: true
data_root: "./data"
batch_size: 128
num_workers: 2
use_amp: true
amp_dtype: float16
grad_scaler_init_scale: 1024

log_filename: "train.log"
# 로그 상세도 ― "INFO" | "DEBUG"
log_level: "DEBUG"
log_step_interval: 100     # 100 step마다 detail 로그

save_checkpoint: true
ckpt_dir: "./checkpoints"

# ────────────── Logging & Dashboard ──────────────
log_all_hparams: true        # 모든 하이퍼파라미터 표 형태 출력
disable_tqdm: false          # 진행바 ON

# ────────────── Weights & Biases ────────────────
use_wandb: true
wandb_entity: "kakamy0820-yonsei-university"
wandb_project: "kd_monitor"
# run name 미지정 시 exp_id 가 그대로 사용됨
wandb_run_name: ""
# (선택) 키를 여기서 직접 지정하면 `wandb login` 생략 가능
wandb_api_key: ""            # ""이면 env/W&B login 사용

finetune_ckpt1: "./checkpoints/teacher1_finetuned.pth"
finetune_ckpt2: "./checkpoints/teacher2_finetuned.pth"

# === Information Bottleneck MBM ===
mbm_type: ib_mbm
use_ib: true
ib_beta: 0.01   # β in IB loss
mbm_query_dim: 2048

# === Continual-Learning ===
cl_mode: false          # true → Split-CIFAR etc.
num_tasks: 10
replay_ratio: 0.5
lambda_ewc: 0.4
