# configs/default.yaml

device: "cuda"
seed: 42
deterministic: true
dataset_name: "cifar100"
small_input: true
data_root: "./data"
batch_size: 128
num_workers: 2
use_amp: true
amp_dtype: float16
grad_scaler_init_scale: 1024

log_filename: "train.log"
# 로그 상세도 ― "INFO" | "DEBUG"
log_level: "DEBUG"

######################
# WandB 사용 여부
######################
use_wandb: true
wandb_project: "kd_monitor"
wandb_entity: "kakamy0820-yonsei-university"   # ← 필요 없으면 빈칸
# run name 미지정 시 exp_id 가 그대로 사용됨
wandb_run_name: ""

# 모든 하이퍼파라미터 표 형태로 로그 파일에 출력
log_all_hparams: true
save_checkpoint: true
ckpt_dir: "./checkpoints"

finetune_ckpt1: "./checkpoints/teacher1_finetuned.pth"
finetune_ckpt2: "./checkpoints/teacher2_finetuned.pth"

# === Information Bottleneck MBM ===
mbm_type: ib_mbm
use_ib: true
ib_beta: 0.01   # β in IB loss

# === Continual-Learning ===
cl_mode: false          # true → Split-CIFAR etc.
num_tasks: 10
replay_ratio: 0.5
lambda_ewc: 0.4
