# configs/finetune/convnext_l_imagenet32.yaml
# ConvNeXt-L 파인튜닝 (ImageNet‑32, 32×32 입력, 1000 클래스) - 성능 개선 버전
# @package _global_

defaults:
  - /dataset: imagenet32
  - _self_

teacher_type: convnext_l
small_input: true
teacher_pretrained: true
teacher_ckpt_init: null

# ─── 성능 개선을 위한 하이퍼파라미터 ──────────────────────────
finetune_epochs: 120        # 더 긴 학습 시간
finetune_lr: 4e-5          # 더 낮은 학습률 (안정성)
finetune_weight_decay: 1.2e-2  # 더 강한 정규화
finetune_use_cutmix: true
finetune_cutmix_alpha: 1.0
label_smoothing: 0.7       # 더 강한 label smoothing

warmup_epochs: 10          # 더 긴 warmup
min_lr: 4e-7

# ─── 고급 스케줄링 설정 ──────────────────────────────────────
scheduler_type: cosine_warm_restarts  # 더 효과적인 스케줄러
restart_period: 30         # 30 에포크마다 재시작
restart_multiplier: 0.8    # 재시작 시 LR을 0.8배로 감소
early_stopping_patience: 15    # 더 긴 patience
early_stopping_min_delta: 0.05  # 더 작은 개선도 요구

seed: 42
device: cuda
log_level: INFO
deterministic: true

# ─── 출력 경로 ────────────────────────────────────────────────
results_dir:        experiments/finetune/convnext_l_imagenet32_ft_improved/results
exp_id:             convnext_l_imagenet32_improved

# 추후 save 할 체크포인트 경로
finetune_ckpt_path: checkpoints/teachers/convnext_l_imagenet32_improved.pth

# Hydra 의 편의를 위해 명시(로더에서 사용)
dataset_name: imagenet32
data_root: ./data
batch_size: 64 # A6000 GPU에서 ConvNeXt-L용
use_amp: true 