# configs/method/vib/standard.yaml

method: vib
teacher_iters: 5        # 또는 10 (GPU 여유 따라)
mbm_type: GATE
z_dim: 512
beta_bottleneck: 1e-3       # KL loss 가 1.0 이상으로 올라오도록 살짝 강화
proj_hidden_dim: 1024
proj_use_bn: true
log_kl: false
student_iters: 100            #  ≈ 1시간 이내에 결과 확인
student_lr       : 3e-3        # 100 epochs
min_lr_ratio_student: 0.05
lr_schedule: cosine
lr_warmup_epochs: 0          # warm‑up 제거
use_amp: true

# ─ Teacher inference only ─
teacher_lr          : 1e-3
teacher_weight_decay: 5e-4
gate_dropout        : 0.1
teacher_dropout_p   : 0.1
teacher1_ckpt       : /home/suyoung425/ASMB_KD/checkpoints/resnet152_ft.pth
# 두 번째 교사 제거 → ensemble 단일화
# ─ Adaptive Gate 옵션 추가 ─
adaptive_gate   : true
gate_hidden_dim : 128

# ─ KD 스케줄 ─
## KD 스케줄을 **더 일찍/강하게** 시작
kd_alpha_init    : 0.2        # 첫 epoch부터 KD 20 %
kd_warmup_frac   : 0.0        # warm‑up 제거
kd_alpha_final   : 0.7
kd_T_init        : 3.0        # 초기 T↓  → KL scale 감소
kd_T_final       : 1.0        # 마지막엔 거의 CE 레벨
kd_sched_pow     : 1.0        # 선형 스케줄 (완만한 변곡)
kd_schedule_granularity: epoch  # step → epoch
ce_alpha      : 1.5

# ─ Latent 정렬 ─
latent_alpha        : 0.50          # 상대적으로 중요도 ↑
latent_warmup_frac  : 0.3
latent_mse_weight   : 0.7
latent_angle_weight : 0.3
cw_mse_eps: 1e-6                 # 확신도‑가중 MSE용 ε (분모 보호)
latent_norm: "dim"          # "dim" | "sqrt" | "none"
latent_clamp_min: -6
latent_clamp_max: 2

# ─ Feature distillation ─
feat_layers   : [2, 3]
feat_weights  : [0.5, 0.5]
feat_loss_weight: [0.2, 0.2, 0.2]   # 단계별(3‑step) 스케줄
                                    # teacher_iters > 0 일 때만 feat_loss 사용

# ───── 추가 override (base.yaml 값을 덮어쓰기) ─────
mixup_alpha          : 0.2          # 표준 MixUp 활성화
cutmix_alpha_distill : 0.0          # CutMix는 OFF (충돌 방지)
ema_decay            : 0.995      # EMA가 조금 더 빠르게 student 추종

# ─ Optimization ─
grad_clip_norm_init: 0.5        # 초기 gradient 폭 줄이기
grad_clip_norm_final: 0.0
grad_clip_warmup_frac: 0.10     # 더 오래 유지

# ─ DataLoader ─
num_workers: 4         # 컨테이너 4‑CPU 기준
