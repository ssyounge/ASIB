# configs/method/vib/standard.yaml

method: vib
teacher_iters: 5        # 또는 10 (GPU 여유 따라)
mbm_type: GATE
z_dim: 512
beta_bottleneck: 1e-3       # KL loss 가 1.0 이상으로 올라오도록 살짝 강화
proj_hidden_dim: 1024
proj_use_bn: true
log_kl: false
student_iters: 100            #  ≈ 1시간 이내에 결과 확인
student_lr       : 3e-3        # 100 epochs
min_lr_ratio_student: 0.05
lr_schedule: cosine
lr_warmup_epochs: 0          # warm‑up 제거
use_amp: true

# ─ Teacher inference only ─
teacher_lr          : 1e-3
teacher_weight_decay: 5e-4
gate_dropout        : 0.1
teacher_dropout_p   : 0.1
teacher1_ckpt       : /home/suyoung425/ASMB_KD/checkpoints/resnet152_ft.pth
# 두 번째 교사 제거 → ensemble 단일화
# ─ Adaptive Gate 옵션 추가 ─
adaptive_gate   : true
gate_hidden_dim : 128

# ─ KD 스케줄 ─
kd_alpha_init    : 0.0        # first 5 % epochs KD OFF
kd_warmup_frac   : 0.10       # linear warmup until 10 %
kd_alpha_final   : 0.6
kd_T_init        : 3.5
kd_T_final       : 1.5
kd_sched_pow     : 0.9
kd_schedule_granularity: epoch  # step → epoch
ce_alpha      : 1.0

# ─ Latent 정렬 ─
latent_alpha        : 0.35
latent_warmup_frac  : 0.3
latent_mse_weight   : 0.7
latent_angle_weight : 0.3
cw_mse_eps: 1e-6                 # 확신도‑가중 MSE용 ε (분모 보호)
latent_norm: "dim"          # "dim" | "sqrt" | "none"
latent_clamp_min: -6
latent_clamp_max: 2

# ─ Feature distillation ─
feat_layers   : [2, 3]
feat_weights  : [0.5, 0.5]
feat_loss_weight: [0.0, 0.0, 0.0]   # 속도 ↑; 나중에 다시 켜도 됨
                                    # teacher_iters > 0 일 때만 feat_loss 사용

# ───── 추가 override (base.yaml 값을 덮어쓰기) ─────
cutmix_alpha_distill: 1.0        # 강한 CutMix 유지
ema_decay           : 0.999      # EMA 관성 소폭 ↑

# ─ Optimization ─
grad_clip_norm_init: 1.0        # clip 완화 + 해제 시점 당김
grad_clip_norm_final: 0.0
grad_clip_warmup_frac: 0.05

# ─ DataLoader ─
num_workers: 4         # 컨테이너 4‑CPU 기준
