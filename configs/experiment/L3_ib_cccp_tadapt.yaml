defaults:
  - /base
  - /model/teacher@experiment.teacher1: convnext_l
  - /model/teacher@experiment.teacher2: resnet152
  - /model/student@experiment.model.student: resnet101_scratch
  - _self_

experiment:
  results_dir: experiments/ablation/ib_cccp_tadapt/results
  exp_id: L3_ib_cccp_tadapt
  teacher1_ckpt: checkpoints/teachers/convnext_l_cifar100.pth
  teacher2_ckpt: checkpoints/teachers/resnet152_cifar100.pth

  dataset:
    batch_size: 64
    num_workers: 4
    data_aug: 1

  # Stages/Epochs (BS=64 풀-런)
  num_stages: 2
  student_epochs_per_stage: [20, 20]
  teacher_adapt_epochs: 14
  ib_epochs_per_stage: 12

  # KD/타깃
  kd_target: synergy
  kd_alpha: 0.25
  tau: 8.0
  teacher_adapt_kd_warmup: 5

  # IB
  use_ib: true
  ib_beta: 0.0003
  ib_beta_warmup_epochs: 5
  use_vib_synergy_head: false

  # CCCP
  use_cccp: true
  cccp_nt: 1
  cccp_ns: 1

  # T‑Adapt: 교사 백본 동결 + distillation adapter만
  use_teacher_finetuning: false
  train_distill_adapter_only: true
  teacher_lr: 3e-06
  teacher_weight_decay: 1e-4
  ib_mbm_lr_factor: 300

  # IB‑MBM 용량(시너지 표현력↑)
  ib_mbm_query_dim: 512
  ib_mbm_out_dim: 768
  ib_mbm_n_head: 4
  ib_mbm_feature_norm: l2

  # Optimizer/학습률
  optimizer: adamw
  student_lr: 0.001
  student_weight_decay: 0.0003
  b_step_lr: 0.001
  b_step_momentum: 0.9
  b_step_nesterov: true
  grad_clip_norm: 0.5

  # CCCP 경로에서도 IB/Head는 충분한 lr (teacher_lr가 0이 아니므로 factor 사용)
  a_step_lr: 0.001
  a_step_weight_decay: 0.0001

  # 스케줄
  schedule:
    type: cosine
    lr_warmup_epochs: 5
    min_lr: 1e-5

  # AMP
  use_amp: true
  amp_dtype: bfloat16

  # Distillation Adapter
  use_distillation_adapter: true
  distill_out_dim: 512
  feat_kd_key: distill_feat
  ce_label_smoothing: 0.1

  compute_teacher_eval: true


