# configs/hparams.yaml
# Default hyperparameters for the batch scripts.

# ===============================================================
# 1. General & Experiment Settings
# ===============================================================
device: "cuda"
seed: 42
deterministic: true
dataset_name: "cifar100"
data_root: "./data"
batch_size: 128
num_workers: 2
data_aug: 1
small_input: true

# ===============================================================
# 2. Model & Checkpoint Settings
# ===============================================================
# For ASMB Multi-Teacher
# Available teacher architectures: resnet101, resnet152, efficientnet_b2, swin_tiny
# Lists used by run_experiments.sh when looping
teacher1_list: ["resnet152"]
teacher2_list: ["efficientnet_b2", "swin_tiny"]
student_list: ["resnet_adapter", "efficientnet_adapter", "swin_adapter", "convnext_tiny"]
method_list: ["asmb", "vanilla_kd", "dkd", "crd", "at", "fitnet"] # "asmb", "vanilla_kd", "dkd", "crd", "at", "fitnet"

# Partial-freeze / adapter options
use_partial_freeze: true
teacher1_use_adapter: 0
teacher1_bn_head_only: 0
teacher2_use_adapter: 0
teacher2_bn_head_only: 0

# Additional teacher options
finetune_partial_freeze: false
efficientnet_dropout: 0.3
swin_adapter_dim: 64  # hidden size for student_swin_adapter

# ===============================================================
# 3. Fine-tuning Hyperparameters
# ===============================================================
finetune_epochs: 200
finetune_lr: 0.001
finetune_weight_decay: 0.0005
finetune_use_cutmix: true
finetune_cutmix_alpha: 1.0
finetune_alpha: 1.0

# ===============================================================
# 4. Distillation Hyperparameters
# ===============================================================
teacher_lr: 3e-4
student_lr: 5e-4
teacher_weight_decay: 1e-4
student_weight_decay: 3e-4
ce_alpha: 0.3
kd_alpha: 0.4

# Learning rate schedule
lr_schedule: "cosine"       # "step" or "cosine"
teacher_step_size: 10
teacher_gamma: 0.1
student_step_size: 10
student_gamma: 0.1

# Temperature scheduling
temperature_schedule: "linear_decay"
tau_start: 6.0
tau_end: 2.0
tau_decay_epochs: 40
mbm_out_dim: 1024
mbm_reg: 2e-5         # default sweep value
reg_lambda: 5e-5      # teacher L2 regularisation
mbm_dropout: 0.1      # dropout prob inside MBM
head_dropout: 0.0     # dropout prob for synergy head

# Sweep/loop variables
# N_STAGE_LIST accepts a space-separated list of stage counts,
# e.g. "2 3 4 5" to run multiple values in a batch script.
n_stage_list: 5
sc_alpha_list: "0.6" # 0.3 0.6
# Additional sweep lists
hybrid_beta_list: "0.1"       # e.g. "0.1 0.5 0.9"
# Epoch sweep lists
teacher_adapt_epochs_list: "10"
student_epochs_per_stage_list: "40"
# Teacher adaptation
teacher_adapt_alpha_kd: 1.0
mbm_lr_factor: 2.0
use_distillation_adapter: true
distill_hidden_dim: 0  # 0 -> use half of teacher feature dim
distill_out_dim: 0     # 0 -> use quarter of teacher feature dim

# MBM options
mbm_type: "LA"
mbm_r: 4
mbm_n_head: 1
mbm_learnable_q: true
mbm_query_dim: 0

# Misc
grad_clip_norm: 1.0
proj_normalize: true

# ===============================================================
# 5. Other Advanced Options
# ===============================================================
# --- Regularization & Augmentation ---
label_smoothing: 0.0

# --- Feature KD ---
feat_kd_alpha: 0.3
feat_kd_key: "feat_2d"
feat_kd_norm: "none"

# --- Disagreement weighting ---
use_disagree_weight: false
disagree_mode: "pred"
disagree_lambda_high: 1.0
disagree_lambda_low: 1.0

