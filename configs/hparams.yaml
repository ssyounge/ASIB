# configs/hparams.yaml
# Default hyperparameters for the batch scripts.

# ===============================================================
# 1. General & Experiment Settings
# ===============================================================
device: "cuda"
seed: 42
deterministic: true
dataset_name: "cifar100"
data_root: "./data"
batch_size: 128
num_workers: 2
data_aug: 1
small_input: true

# ===============================================================
# 2. Model & Checkpoint Settings
# ===============================================================
# For ASMB Multi-Teacher
# Available teacher architectures: resnet101, resnet152, efficientnet_b2, swin_tiny
# Lists used by run_experiments.sh when looping
teacher1_list: ["resnet152"]
teacher2_list: ["efficientnet_b2", "swin_tiny"]
student_list: ["resnet_adapter"]      # 첫 실험은 하나로 충분
method_list: ["asmb", "vanilla_kd", "dkd", "crd", "at", "fitnet"] # "asmb", "vanilla_kd", "dkd", "crd", "at", "fitnet"

# Partial-freeze / adapter options
use_partial_freeze: false      # 첫 실험은 full fine-tune 로 성능 ceiling 파악
student_freeze_level: 0        # head 만 고정 (실제로는 전체 학습)
student_use_adapter: true
teacher1_use_adapter: 0
teacher1_bn_head_only: 0
teacher2_use_adapter: 0
teacher2_bn_head_only: 0

# Additional teacher options
finetune_partial_freeze: false
efficientnet_dropout: 0.3
swin_adapter_dim: 64  # hidden size for student_swin_adapter

# ===============================================================
# 3. Fine-tuning Hyperparameters
# ===============================================================
finetune_epochs: 10                  # teacher 성능 ↑ → distill 성능 ↑
finetune_lr: 3e-4                    # backbone 을 크게 흔들지 않는 범위
finetune_weight_decay: 0.0005
finetune_use_cutmix: true
finetune_cutmix_alpha: 1.0
finetune_alpha: 1.0

# ===============================================================
# 4. Distillation Hyperparameters
# ===============================================================
teacher_lr: 3e-4
student_lr: 0.001              # full fine-tune 에 맞춰 2× 상승
teacher_weight_decay: 1e-4
student_weight_decay: 3e-4
ce_alpha: 0.25
kd_alpha: 0.55                 # logit KD 비중 ↑ (freeze 해제 됐으므로 수용력↑)
cutmix_alpha_distill: 0.0  # CutMix alpha used during distillation

# Learning rate schedule
lr_schedule: "cosine"       # "step" or "cosine"
lr_warmup_epochs: 3                   # warm-up 3 ep (60 ep × 5 %)
teacher_step_size: 10
teacher_gamma: 0.1
student_step_size: 10
student_gamma: 0.1

# Temperature scheduling
temperature_schedule: "linear_decay"
tau_start: 6.0
tau_end: 2.0
tau_decay_epochs: 4          # stage 수와 동일하게
mbm_out_dim: 2048            # student feat_dim 과 일치
mbm_reg: 2e-5         # default sweep value
ib_beta: 0.005                 # 정보량 제약 완화
reg_lambda: 2e-5             # L2 강도 살짝 완화
mbm_dropout: 0.1      # dropout prob inside MBM
head_dropout: 0.0     # dropout prob for synergy head

# Sweep/loop variables
# N_STAGE_LIST accepts a space-separated list of stage counts,
# e.g. "2 3 4 5" to run multiple values in a batch script.
num_stages: 4
n_stage_list: 4
sc_alpha_list: "0.6" # 0.3 0.6
# Additional sweep lists
hybrid_beta_list: "0.1"       # e.g. "0.1 0.5 0.9"
# Epoch sweep lists
teacher_adapt_epochs_list: "3"
student_epochs_per_stage_list: "12"   # full fine-tune 는 수렴 빠르므로 12 ep 로 균형
# Teacher adaptation
teacher_adapt_alpha_kd: 1.0
mbm_lr_factor: 2.0
use_distillation_adapter: true
# Distillation adapter dimension (통일)
distill_hidden_dim: 1024  # same for 모든 교사
distill_out_dim: 512      # 모든 교사 512-D

# MBM options (타입은 default.yaml에서 상속/고정)
mbm_r: 4
mbm_n_head: 1
mbm_learnable_q: true
mbm_query_dim: 0

# Misc
grad_clip_norm: 1.0

# ===============================================================
# 5. Other Advanced Options
# ===============================================================
# --- Regularization & Augmentation ---
label_smoothing: 0.0
mixup_alpha: 0.2

# --- Feature KD ---
feat_kd_alpha: 0.15            # logit KD 증가에 따라 비중 소폭 ↓
feat_kd_key: "feat_2d"
feat_kd_norm: "none"
rkd_loss_weight: 1.0
rkd_gamma: 0.6

# --- Disagreement weighting ---
use_disagree_weight: true
disagree_mode: "both_wrong"
disagree_lambda_high: 1.2
disagree_lambda_low: 0.8

