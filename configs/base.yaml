# configs/base.yaml

dataset_root: ./data
imagenet100_root: ./data/imagenet100
num_classes: 100
student_type: convnext_tiny
teacher1_ckpt: checkpoints/resnet152_ft.pth
# ─ 두 번째 교사 제거(단일 teacher 운용) ─
teacher2_ckpt: checkpoints/efficientnet_b2_ft.pth
teacher1_type: resnet152
teacher2_type: efficientnet_b2

batch_size : 128
replay_ratio: 0.5
num_workers: 4                   # 컨테이너 4‑CPU 기준
persistent_workers: true
disable_tqdm: true           

device     : cuda
checkpoint_dir: checkpoints
results_dir   : results

optimizer  : adamw
## 조금 높은 LR + 더 긴 warm‑up
student_lr       : 5e-4            # CE-warm-start 미세조정
min_lr_ratio_student: 0.01  # cosine 최저 LR ↓
student_warmup_epochs: 10    # 네트워크 안정화 후 본격 학습
lr_schedule: cosine
student_iters: 250          # 학습량 2.5×

# ───────────── Teacher-cache 옵션 ─────────────
# cache/*.pt 은 tools/build_teacher_cache.py 로 한-번만 만들어 두면 됩니다.
use_teacher_cache : false
teacher_cache_path: cache/res152_train.pt     # ← 캐시 파일
# (저장된 key 중 필요한 것만 골라서 GPU 로 복사)
teacher_cache_items: [logits, z]

# 캐시 모드에선 feature-distill 을 끄는 편이 빠릅니다.
feat_loss_weight: [0.0, 0.0, 0.0]
randaug_N: 2
randaug_M        : 7          # 9 → 7   (세기 완화)
mixup_alpha      : 0.1            # KD 단계에선 MixUp 강도 줄임
cutmix_alpha_distill: 0.1   # Noise-robust but 보수적으로

use_amp   : true
amp_dtype : float16
use_ema   : true
ema_decay : 0.99
ema_initial_decay: 0.90
ema_warmup_iters : 5

eval_after_train : true
grad_clip_norm   : 1.0
# ─ Grad‑clip 스케줄 옵션 ─
grad_clip_norm_init:   0.5   # 초기 gradient 폭 줄이기
grad_clip_norm_final:  0.0   # 끝 값(0 = 해제)
grad_clip_warmup_frac: 0.10  # 더 오래 유지
grad_scaler_init_scale: 1024

patch_stride2 : false
seed: 42
student_weight_decay: 5e-4
# ⚠️ 여기서 지정하면 모든 시나리오가 덮어쓰기 당함 → 삭제
# student_ce_ckpt: checkpoints/convnext_ce77.pth
use_ewc: false
ewc_lambda: 30.0
ewc_samples: 1024
ewc_online: false
ewc_decay: 1.0

##############################
# 기본 Distillation 방법 기본값
#  · CE‑파인튜닝 Job   → 그대로 'ce'
#  · KD  Job          → 아래 YAML에서 'vib' 등으로 다시 덮어씀
method: ce
train_mode: standard

# ───── Logging options ─────
exp_name: ibkd
wandb_project: kd_monitor
wandb_run_name: run_001
tb_log_dir: runs/kd_monitor
log_interval: 100        # 100 mini-batches마다 한 줄
