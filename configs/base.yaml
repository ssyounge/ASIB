# configs/base.yaml

# ---------------------------------------------------------------
#  Global defaults – 공통 설정 / 가장 먼저 로드
# ---------------------------------------------------------------
defaults:
  - dataset: cifar100                # 데이터셋·로더 관리
  - model/teacher: resnet152         # 교사 모델
  - model/student: resnet152_pretrain # 학생 모델
  - method: asmb                     # KD 방법
  - schedule: cosine                 # LR 스케줄
  - _self_                           # (항상 마지막)

# ---------- 실험 공통 ----------
device: cuda
seed: 42
deterministic: true
use_amp: true
amp_dtype: float16
grad_scaler_init_scale: 1024

# Teacher identifiers (used by synergy/eval scripts)
teacher1_type: resnet152
teacher2_type: resnet152

# ---------- Data ----------
batch_size: 128            # dataset 그룹 값으로 덮어써도 됨
num_workers: 4

# ---------- Logging ----------
log:
  filename: "train.log"
  level: DEBUG
  step_interval: 100
disable_tqdm: false
log_all_hparams: true

# ---------- Checkpoint ----------
save_checkpoint: true
ckpt_dir: "./checkpoints"

# ---------- W&B ----------
wandb:
  use: false
  entity: "kakamy0820-yonsei-university"
  project: "kd_monitor"
  run_name: ""              # 비우면 exp_id 사용
  api_key: ""               # "" → wandb login 로드

# ---------- MBM / Information‑Bottleneck ---------------------------------
#  ※ 코드( main.py, trainer_*.py ) 는 중첩 dict 대신 **flat key** 를 조회합니다.
#    그래서 기존 `mbm:` 블록을 없애고 flat alias 로 교체합니다.

# 모듈 종류  ('la', 'ib_mbm', 'mlp')
mbm_type: ib_mbm

# IB 옵션
use_ib: true
ib_beta: 0.01                     # 최종 β
beta_schedule: [0.0, 0.01, 5]     # 0→0.01 로 5 epoch linear warm‑up

# 차원 설정
mbm_query_dim: 2048               # student feat dim (ResNet‑152)
mbm_d_emb: 512                    # ★ 내부 embedding 크기 변경 (256→512)
mbm_out_dim: 512                  # synergy‑head 입력 dim (= d_emb)

# ---------- Distillation Adapter 기본 ----------
use_distillation_adapter: false
# teacher adapter MLP 크기 (선택)
distill_hidden_dim: 512
distill_out_dim: 512

# ---------- Distillation stage 기본 ----------
num_stages: 3
student_epochs_per_stage: 15
teacher_adapt_epochs: 2
grad_clip_norm: 1.0
