# configs/base.yaml

dataset_root: ./data
imagenet100_root: ./data/imagenet100
num_classes: 100
student_type: convnext_tiny
teacher1_ckpt: checkpoints/resnet152_ft.pth
# ─ 두 번째 교사 제거(단일 teacher 운용) ─
teacher2_ckpt: checkpoints/efficientnet_b2_ft.pth
teacher1_type: resnet152
teacher2_type: efficientnet_b2

batch_size : 128
replay_ratio: 0.5
num_workers: 4                   # 컨테이너 4‑CPU 기준
persistent_workers: true
disable_tqdm: true           

device     : cuda
checkpoint_dir: checkpoints
results_dir   : results

optimizer  : adamw
## 조금 높은 LR + 더 긴 warm‑up
student_lr       : 4e-3
min_lr_ratio_student: 0.02   # cosine 최저 LR ↓
student_warmup_epochs: 10    # 네트워크 안정화 후 본격 학습
lr_schedule: cosine
student_iters: 50             # 기본 에폭 단축

# ───────────── Teacher-cache 옵션 ─────────────
# cache/*.pt 은 tools/build_teacher_cache.py 로 한-번만 만들어 두면 됩니다.
use_teacher_cache : false
teacher_cache_path: cache/res152_train.pt     # ← 캐시 파일
# (저장된 key 중 필요한 것만 골라서 GPU 로 복사)
teacher_cache_items: [logits, z]

# 캐시 모드에선 feature-distill 을 끄는 편이 빠릅니다.
feat_loss_weight: [0.0, 0.0, 0.0]
randaug_N: 2
randaug_M        : 7          # 9 → 7   (세기 완화)
mixup_alpha      : 0.1        # 0.20 → 0.10
cutmix_alpha_distill: 0.5

use_amp   : true
amp_dtype : float16
use_ema   : true
ema_decay : 0.99
ema_initial_decay: 0.90
ema_warmup_iters : 5

eval_after_train : true
grad_clip_norm   : 1.0
# ─ Grad‑clip 스케줄 옵션 ─
grad_clip_norm_init:   0.5   # 초기 gradient 폭 줄이기
grad_clip_norm_final:  0.0   # 끝 값(0 = 해제)
grad_clip_warmup_frac: 0.10  # 더 오래 유지
grad_scaler_init_scale: 1024

patch_stride2 : false
seed: 42
student_weight_decay: 5e-4
student_ce_ckpt: student_ce_best.pth
use_ewc: false
ewc_lambda: 30.0
ewc_samples: 1024
ewc_online: false
ewc_decay: 1.0

# ───── Logging options ─────
exp_name: ibkd
wandb_project: kd_monitor
wandb_run_name: run_001
tb_log_dir: runs/kd_monitor
