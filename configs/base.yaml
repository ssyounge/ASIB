# configs/base.yaml
defaults:
  - dataset@experiment.dataset: cifar100
  - schedule@experiment.schedule: cosine
  - experiment/method@experiment.method: asib
  - _self_

experiment:
  device: cuda
  seed: 42
  small_input: true

  results_dir: experiments/default/results
  exp_id: default

  dataset:
    batch_size: 64
    num_workers: 8  # 더 많은 워커로 데이터 로딩 속도 향상

  num_stages: 1
  student_epochs_per_stage: [5]
  teacher_adapt_epochs: 0
  use_partial_freeze: false
  use_channels_last: true

  use_amp: false
  amp_dtype: bfloat16  # bfloat16이 더 안정적이고 빠름 (A100/H100에서)

  use_ib: false
  ib_beta: 0.0
  ib_beta_warmup_epochs: 0
  ib_mbm_feature_norm: l2
  ib_mbm_reg_lambda: 0.0

  a_step_lr: 1.0e-4
  a_step_weight_decay: 1.0e-4
  b_step_lr: 1.0e-1
  b_step_weight_decay: 3.0e-4
  b_step_momentum: 0.9
  b_step_nesterov: true
  grad_clip_norm: 0

  ce_alpha: 0.3
  kd_ens_alpha: 0.0

  use_cccp: false
  tau: 4.0

  # Teacher evaluation (초반 평가 스킵으로 속도 향상)
  compute_teacher_eval: false

  # Teacher fine-tuning control (교사 백본 학습 제어)
  use_teacher_finetuning: false  # false: 교사 백본 고정 (기본값)
  train_distill_adapter_only: false  # true: distillation adapter만 학습

  # Disagreement calculation optimization
  disagreement_max_samples: 2000  # 전체 대신 샘플링으로 속도 향상 (None: 전체 사용)
  disagreement_max_batches: 10  # 배치 수 제한 (수 분 → 수 초)

  # Legacy trainer compatibility keys (do not remove)
  teacher_lr: 0.0
  teacher_weight_decay: 0.0
  student_lr: 0.05
  student_weight_decay: 0.0003
  
  # KD warmup and loss stabilization
  teacher_adapt_kd_warmup: 0
  use_loss_clamp: false
  loss_clamp_max: 100.0
  # Single-teacher SOTA default index (used only when kd_target: teacher)
  kd_teacher_index: 0
