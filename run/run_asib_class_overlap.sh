#!/usr/bin/env bash
#SBATCH --job-name=asib_class_overlap
#SBATCH --partition=base_suma_rtx3090
#SBATCH --gres=gpu:1
#SBATCH --cpus-per-task=4
#SBATCH --mem=16G
#SBATCH --time=72:00:00
#SBATCH --output=experiments/logs/class_overlap_%j.log
#SBATCH --error=experiments/logs/class_overlap_%j.err
# ---------------------------------------------------------
# ASIB Class Overlap Analysis ì‹¤í—˜
# 100% Class Overlap ìƒí™©ì—ì„œì˜ ASIB ì„±ëŠ¥ ë¶„ì„
# ---------------------------------------------------------
set -euo pipefail
trap 'echo "âŒ Job failed at $(date)"; exit 1' ERR

# Python í™˜ê²½ ì„¤ì •
echo "ğŸ”§ Setting up Python environment..."
export PATH="$HOME/anaconda3/envs/tlqkf/bin:$PATH"
export HYDRA_FULL_ERROR=1
export OMP_NUM_THREADS=${OMP_NUM_THREADS:-4}
export MKL_NUM_THREADS=${MKL_NUM_THREADS:-4}
echo "âœ… Python environment setup completed"
echo ""

# 1) ë¦¬í¬ ìµœìƒìœ„ë¡œ ì´ë™
ROOT="$(git rev-parse --show-toplevel)"
cd "$ROOT"

# 2) PYTHONPATH ì¶”ê°€
export PYTHONPATH="${ROOT}:${PYTHONPATH:-}"

# Ensure log directory exists
mkdir -p "$ROOT/experiments/logs" || true

# 3) GPU í• ë‹¹ í™•ì¸ ë° ì„¤ì •
echo "ğŸ” Checking GPU allocation..."
if [ -n "$SLURM_GPUS_ON_NODE" ]; then
    # GPU ì¸ë±ìŠ¤ë¥¼ 0ë¶€í„° ì‹œì‘í•˜ë„ë¡ ì¡°ì •
    if [ "$SLURM_GPUS_ON_NODE" = "1" ]; then
        export CUDA_VISIBLE_DEVICES=0
        echo "âœ… CUDA_VISIBLE_DEVICES set to: 0 (mapped from SLURM_GPUS_ON_NODE=1)"
    else
        export CUDA_VISIBLE_DEVICES=0
        echo "âœ… CUDA_VISIBLE_DEVICES set to: 0 (default for any GPU allocation)"
    fi
else
    echo "âš ï¸  SLURM_GPUS_ON_NODE not set, using default GPU 0"
    export CUDA_VISIBLE_DEVICES=0
fi

# CUDA ì»¨í…ìŠ¤íŠ¸ ì´ˆê¸°í™” (segmentation fault ë°©ì§€)
export CUDA_LAUNCH_BLOCKING=${CUDA_LAUNCH_BLOCKING:-0}
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True

# PyTorch CUDA ë¼ì´ë¸ŒëŸ¬ë¦¬ ê²½ë¡œ ê°€ë“œ
TORCH_LIB_DIR="$HOME/anaconda3/envs/tlqkf/lib/python3.12/site-packages/torch/lib"
if [ -d "$TORCH_LIB_DIR" ]; then
  export LD_LIBRARY_PATH="$TORCH_LIB_DIR:$LD_LIBRARY_PATH"
  export CUDA_HOME="$TORCH_LIB_DIR"
fi

# PyTorch CUDA ì„¤ì •
export TORCH_CUDA_ARCH_LIST="8.6"

# CUDA í™˜ê²½ë³€ìˆ˜ (PyTorch ë‚´ì¥ CUDA ì‚¬ìš©)
export CUDA_PATH="${CUDA_HOME:-${CUDA_PATH:-}}"
export CUDA_ROOT="${CUDA_HOME:-${CUDA_ROOT:-}}"

# GPU ì •ë³´ ì¶œë ¥
echo "ğŸ” GPU Information:"
nvidia-smi --query-gpu=name,memory.total,memory.free --format=csv,noheader,nounits



# 4) ASIB Class Overlap Analysis ì‹¤í—˜ë“¤
EXPERIMENTS=(
    "overlap_100"           # 100% Class Overlap Analysis
)

# 5) ê° ì‹¤í—˜ ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰
for exp in "${EXPERIMENTS[@]}"; do
    echo "ğŸš€ Starting ASIB Class Overlap Analysis experiment: $exp"
    echo "=================================================="
    echo "Time: $(date)"
    echo "Experiment: $exp"
    echo "ASIB Class Overlap Analysis"
    echo "=================================================="
    
    # ì‹¤í—˜ ì‹¤í–‰
    python -u main.py -cn="experiment/$exp" \
        "$@"
    
    echo "âœ… Finished ASIB Class Overlap Analysis experiment: $exp"
    echo "=================================================="
    echo "Time: $(date)"
    echo ""
done

echo "ğŸ‰ ASIB Class Overlap Analysis completed!"
echo "ğŸ“ Results saved in: experiments/overlap/results/"
echo "ğŸ“Š All ASIB experiments completed! Ready for analysis" 