# ASIB-CL: Information Bottleneck ê¸°ë°˜ Continual Learning

## ğŸ“– ê°œìš”

ASIB-CLì€ ASIB(Adaptive Sampling Information Bottleneck)ë¥¼ Continual Learning í™˜ê²½ì— ì ìš©í•œ ë°©ë²•ì…ë‹ˆë‹¤. Information Bottleneckì„ í™œìš©í•˜ì—¬ ì•ˆì •ì„±-ê°€ì†Œì„± ë”œë ˆë§ˆ(Stability-Plasticity Dilemma)ë¥¼ í•´ê²°í•˜ëŠ” Class-Incremental Learning ë°©ë²•ì…ë‹ˆë‹¤.

## ğŸ¯ í•µì‹¬ ì•„ì´ë””ì–´

### 1. Information Bottleneck ê¸°ë°˜ ì§€ì‹ ì¦ë¥˜
- **ì•ˆì •ì„±**: ì´ì „ ëª¨ë¸ì˜ ì§€ì‹ì„ ìµœì†Œ ì¶©ë¶„ ì •ë³´ë¡œ ì••ì¶•í•˜ì—¬ ì „ë‹¬
- **ê°€ì†Œì„±**: ë¶ˆí•„ìš”í•œ ì •ë³´ ì „ë‹¬ì„ ì¤„ì—¬ ìƒˆë¡œìš´ íƒœìŠ¤í¬ í•™ìŠµì— ëª¨ë¸ ìš©ëŸ‰ í™•ë³´

### 2. ì•ˆì •ì„±-ê°€ì†Œì„± ë”œë ˆë§ˆ í•´ê²°
- **Î² (IB ì••ì¶• ê°•ë„)**: 0.1ë¡œ ì„¤ì •í•˜ì—¬ ì ì ˆí•œ ì••ì¶• ê°•ë„ ìœ ì§€
- **Knowledge Transfer Loss**: MSE lossë¡œ íŠ¹ì§• ìˆ˜ì¤€ ì§€ì‹ ì „ë‹¬
- **Information Compression Loss**: KL divergenceë¡œ ì •ë³´ ì••ì¶• ìœ ë„

### 3. Class-IL ì‹œë‚˜ë¦¬ì˜¤ ìµœì í™”
- **ë‹¨ì¼ ê³µìœ  í—¤ë“œ**: ëª¨ë“  í´ë˜ìŠ¤ë¥¼ êµ¬ë¶„í•˜ëŠ” ë‹¨ì¼ ë¶„ë¥˜ê¸°
- **ì´ì „ ëª¨ë¸ êµì‚¬**: Oracle êµì‚¬ ëŒ€ì‹  ì´ì „ íƒœìŠ¤í¬ ëª¨ë¸ì„ êµì‚¬ë¡œ ì‚¬ìš©
- **í‘œì¤€ CL í”„ë¡œí† ì½œ ì¤€ìˆ˜**: ë¯¸ë˜ ë°ì´í„° ì ‘ê·¼ ì—†ì´ ìˆœì°¨ì  í•™ìŠµ

## ğŸ—ï¸ ì•„í‚¤í…ì²˜

```
ì´ì „ ëª¨ë¸ (M_{T-1}) â†’ IB ì¸ì½”ë” â†’ ì••ì¶•ëœ í‘œí˜„ (Z) â†’ IB ë””ì½”ë” â†’ ë³µì›ëœ íŠ¹ì§•
                                                      â†“
í˜„ì¬ ëª¨ë¸ (M_T) â† Knowledge Transfer Loss â† MSE Loss
```

### í•µì‹¬ êµ¬ì„± ìš”ì†Œ

1. **IB ì¸ì½”ë”**: êµì‚¬ íŠ¹ì§•ì„ ì••ì¶•ëœ í‘œí˜„ìœ¼ë¡œ ë³€í™˜
2. **IB ë””ì½”ë”**: ì••ì¶•ëœ í‘œí˜„ì„ ì›ë³¸ íŠ¹ì§• ì°¨ì›ìœ¼ë¡œ ë³µì›
3. **Knowledge Transfer Loss**: í•™ìƒ íŠ¹ì§•ì´ ë³µì›ëœ íŠ¹ì§•ì„ ëª¨ë°©í•˜ë„ë¡ ìœ ë„
4. **Information Compression Loss**: KL divergenceë¡œ ì •ë³´ ì••ì¶• ìœ ë„

## ğŸš€ ì„¤ì¹˜ ë° ì„¤ì •

### 1. PyCIL í”„ë ˆì„ì›Œí¬ ì„¤ì¹˜
```bash
# PyCIL í´ë¡  (ì´ë¯¸ ì™„ë£Œë¨)
git clone https://github.com/LAMDA-CL/PyCIL.git

# ì˜ì¡´ì„± ì„¤ì¹˜
pip install torch torchvision tqdm numpy scipy quadprog POT
```

### 2. ASIB-CL ëª¨ë“ˆ í™•ì¸
```bash
# ASIB-CL ëª¨ë¸ íŒŒì¼ í™•ì¸
ls PyCIL/models/asib_cl.py

# ì„¤ì • íŒŒì¼ í™•ì¸
ls PyCIL/exps/asib_cl.json

# Factory ë“±ë¡ í™•ì¸
grep "asib_cl" PyCIL/utils/factory.py
```

## ğŸ“Š ì‹¤í—˜ ì„¤ì •

### ê¸°ë³¸ ì‹¤í—˜ ì„¤ì • (CIFAR-100)
```json
{
    "convnet_type": "resnet32",
    "dataset": "cifar100",
    "init_cls": 10,
    "increment": 10,
    "memory_size": 2000,
    "memory_per_class": 20,
    "device": [0],
    "num_workers": 8,
    "batch_size": 128,
    "epochs": 170,
    "lr": 0.1,
    "lr_decay": 0.1,
    "milestones": [60, 120, 160],
    "weight_decay": 0.0002,
    "ib_beta": 0.1,
    "topk": 5,
    "seed": 1993,
    "logdir": "./experiments/sota/logs/asib_cl",
    "model_name": "asib_cl"
}
```

### ì£¼ìš” í•˜ì´í¼íŒŒë¼ë¯¸í„°

| íŒŒë¼ë¯¸í„° | ê°’ | ì„¤ëª… |
|---------|-----|------|
| `ib_beta` | 0.1 | IB ì••ì¶• ê°•ë„ (ë†’ì„ìˆ˜ë¡ ë” ê°•í•œ ì••ì¶•) |
| `memory_size` | 2000 | ì´ exemplar ê°œìˆ˜ |
| `memory_per_class` | 20 | í´ë˜ìŠ¤ë‹¹ exemplar ê°œìˆ˜ |
| `init_cls` | 10 | ì²« ë²ˆì§¸ íƒœìŠ¤í¬ì˜ í´ë˜ìŠ¤ ìˆ˜ |
| `increment` | 10 | ê° íƒœìŠ¤í¬ë‹¹ ì¶”ê°€ë˜ëŠ” í´ë˜ìŠ¤ ìˆ˜ |

## ğŸ§ª ì‹¤í—˜ ì‹¤í–‰

### 1. ë‹¨ì¼ ì‹¤í—˜ ì‹¤í–‰
```bash
# ASIB-CL ì‹¤í—˜ ì‹¤í–‰
python PyCIL/main.py --config=PyCIL/exps/asib_cl.json
```

### 2. ë¹„êµ ì‹¤í—˜ ì‹¤í–‰
```bash
# ì „ì²´ ë¹„êµ ì‹¤í—˜ ì‹¤í–‰
python run_asib_cl_experiment.py
```

### 3. ê°œë³„ ë°©ë²• ë¹„êµ
```bash
# Fine-tuning
python PyCIL/main.py --config=PyCIL/exps/finetune.json

# EWC
python PyCIL/main.py --config=PyCIL/exps/ewc.json

# LwF
python PyCIL/main.py --config=PyCIL/exps/lwf.json

# iCaRL
python PyCIL/main.py --config=PyCIL/exps/icarl.json

# DER
python PyCIL/main.py --config=PyCIL/exps/der.json
```

## ğŸ“ˆ ê²°ê³¼ ë¶„ì„

### ì„±ëŠ¥ ì§€í‘œ
- **Average Incremental Accuracy (AIA)**: ëª¨ë“  íƒœìŠ¤í¬ ì™„ë£Œ í›„ í‰ê·  ì •í™•ë„
- **Average Forgetting (AF)**: ì´ì „ íƒœìŠ¤í¬ì— ëŒ€í•œ ì„±ëŠ¥ ë§ê° ì •ë„
- **Forward Transfer**: ìƒˆë¡œìš´ íƒœìŠ¤í¬ í•™ìŠµ ì‹œ ì´ì „ ì§€ì‹ì˜ í™œìš©ë„

### ê²°ê³¼ í™•ì¸
```bash
# ë¡œê·¸ íŒŒì¼ í™•ì¸
tail -f experiments/sota/logs/asib_cl/*.log

# ê²°ê³¼ ë¦¬í¬íŠ¸ ìƒì„±
python run_asib_cl_experiment.py
```

## ğŸ”¬ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹

### IB ì••ì¶• ê°•ë„ (Î²) ì‹¤í—˜
```python
# ë‹¤ì–‘í•œ Î² ê°’ìœ¼ë¡œ ì‹¤í—˜
beta_values = [0.01, 0.05, 0.1, 0.2, 0.5]

for beta in beta_values:
    # ì„¤ì • íŒŒì¼ ìˆ˜ì •
    config["ib_beta"] = beta
    # ì‹¤í—˜ ì‹¤í–‰
```

### ë©”ëª¨ë¦¬ í¬ê¸° ì‹¤í—˜
```python
# ë‹¤ì–‘í•œ ë©”ëª¨ë¦¬ í¬ê¸°ë¡œ ì‹¤í—˜
memory_sizes = [1000, 2000, 3000, 5000]

for memory_size in memory_sizes:
    # ì„¤ì • íŒŒì¼ ìˆ˜ì •
    config["memory_size"] = memory_size
    # ì‹¤í—˜ ì‹¤í–‰
```

## ğŸ“ ì½”ë“œ êµ¬ì¡°

```
PyCIL/
â”œâ”€â”€ models/
â”‚   â””â”€â”€ asib_cl.py          # ASIB-CL ëª¨ë¸ êµ¬í˜„
â”œâ”€â”€ exps/
â”‚   â””â”€â”€ asib_cl.json        # ASIB-CL ì‹¤í—˜ ì„¤ì •
â””â”€â”€ utils/
    â””â”€â”€ factory.py          # ëª¨ë¸ íŒ©í† ë¦¬ (ASIB-CL ë“±ë¡ë¨)

run_asib_cl_experiment.py   # ì‹¤í—˜ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
ASIB_CL_README.md          # ì´ íŒŒì¼
```

## ğŸ¯ í•µì‹¬ êµ¬í˜„ ì„¸ë¶€ì‚¬í•­

### 1. IB ëª¨ë“ˆ ì´ˆê¸°í™”
```python
def _init_ib_modules(self, feature_dim):
    latent_dim = feature_dim // 4  # ì••ì¶•ëœ í‘œí˜„ ì°¨ì›
    
    self._ib_encoder = nn.Sequential(
        nn.Linear(feature_dim, feature_dim // 2),
        nn.ReLU(),
        nn.Linear(feature_dim // 2, latent_dim * 2)  # mu, logvar
    )
    
    self._ib_decoder = nn.Sequential(
        nn.Linear(latent_dim, feature_dim // 2),
        nn.ReLU(),
        nn.Linear(feature_dim // 2, feature_dim)
    )
```

### 2. IB ê¸°ë°˜ ì§€ì‹ ì¦ë¥˜ ì†ì‹¤
```python
def _ib_distillation_loss(self, student_features, teacher_features):
    # êµì‚¬ íŠ¹ì§•ì„ IB ì¸ì½”ë”ë¡œ ì••ì¶•
    ib_output = self._ib_encoder(teacher_features)
    mu, logvar = ib_output.chunk(2, dim=1)
    
    # Reparameterization
    z = self._reparameterize(mu, logvar)
    
    # ì••ì¶•ëœ í‘œí˜„ì„ ë””ì½”ë”ë¡œ ë³µì›
    reconstructed_features = self._ib_decoder(z)
    
    # Knowledge Transfer Loss (ì•ˆì •ì„±)
    knowledge_transfer_loss = F.mse_loss(student_features, reconstructed_features)
    
    # Information Compression Loss (ê°€ì†Œì„±)
    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())
    
    # ì „ì²´ IB ì†ì‹¤
    ib_loss = knowledge_transfer_loss + self._ib_beta * kl_loss
    
    return ib_loss
```

## ğŸ” ë¬¸ì œ í•´ê²°

### ì¼ë°˜ì ì¸ ë¬¸ì œë“¤

1. **CUDA ë©”ëª¨ë¦¬ ë¶€ì¡±**
   ```bash
   # ë°°ì¹˜ í¬ê¸° ì¤„ì´ê¸°
   "batch_size": 64  # 128ì—ì„œ 64ë¡œ ë³€ê²½
   ```

2. **í•™ìŠµì´ ìˆ˜ë ´í•˜ì§€ ì•ŠìŒ**
   ```bash
   # í•™ìŠµë¥  ì¡°ì •
   "lr": 0.05  # 0.1ì—ì„œ 0.05ë¡œ ë³€ê²½
   ```

3. **IB ì••ì¶•ì´ ë„ˆë¬´ ê°•í•¨**
   ```bash
   # Î² ê°’ ì¤„ì´ê¸°
   "ib_beta": 0.05  # 0.1ì—ì„œ 0.05ë¡œ ë³€ê²½
   ```

## ğŸ“š ì°¸ê³  ë¬¸í—Œ

1. **PyCIL**: Zhou, D. W., et al. "PyCIL: A Python Toolbox for Class-Incremental Learning." Science China Information Sciences, 2023.
2. **Information Bottleneck**: Tishby, N., et al. "The information bottleneck method." Allerton Conference, 1999.
3. **Variational Information Bottleneck**: Alemi, A. A., et al. "Deep variational information bottleneck." ICLR, 2017.

## ğŸ¤ ê¸°ì—¬

ASIB-CL êµ¬í˜„ì— ëŒ€í•œ ì§ˆë¬¸ì´ë‚˜ ê°œì„  ì œì•ˆì´ ìˆìœ¼ì‹œë©´ ì´ìŠˆë¥¼ ìƒì„±í•´ ì£¼ì„¸ìš”.

## ğŸ“„ ë¼ì´ì„ ìŠ¤

ì´ í”„ë¡œì íŠ¸ëŠ” MIT ë¼ì´ì„ ìŠ¤ í•˜ì— ë°°í¬ë©ë‹ˆë‹¤. 