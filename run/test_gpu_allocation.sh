#!/usr/bin/env bash
#SBATCH --job-name=test_gpu_allocation
#SBATCH --partition=base_suma_rtx3090
#SBATCH --gres=gpu:1
#SBATCH --cpus-per-task=1
#SBATCH --mem=8G
#SBATCH --time=0:10:00
#SBATCH --output=experiments/logs/test_gpu_%j.log
#SBATCH --error=experiments/logs/test_gpu_%j.err

set -euo pipefail

# Python ÌôòÍ≤Ω ÏÑ§Ï†ï
echo "üîß Setting up Python environment..."
export PATH="$HOME/anaconda3/envs/tlqkf/bin:$PATH"
echo "‚úÖ Python environment setup completed"
echo ""

echo "=== GPU Allocation Test ==="
echo "Time: $(date)"
echo ""

# SLURM ÌôòÍ≤ΩÎ≥ÄÏàò ÌôïÏù∏
echo "üîç SLURM Environment Variables:"
echo "SLURM_GPUS_ON_NODE: ${SLURM_GPUS_ON_NODE:-'Not set'}"
echo "SLURM_GPUS_PER_NODE: ${SLURM_GPUS_PER_NODE:-'Not set'}"
echo "SLURM_GPUS_PER_TASK: ${SLURM_GPUS_PER_TASK:-'Not set'}"
echo "SLURM_JOB_NODELIST: ${SLURM_JOB_NODELIST:-'Not set'}"
echo ""

# GPU Ìï†Îãπ ÌôïÏù∏ Î∞è ÏÑ§Ï†ï
echo "üîç Checking GPU allocation..."
if [ -n "$SLURM_GPUS_ON_NODE" ]; then
    # GPU Ïù∏Îç±Ïä§Î•º 0Î∂ÄÌÑ∞ ÏãúÏûëÌïòÎèÑÎ°ù Ï°∞Ï†ï
    if [ "$SLURM_GPUS_ON_NODE" = "1" ]; then
        export CUDA_VISIBLE_DEVICES=0
        echo "‚úÖ CUDA_VISIBLE_DEVICES set to: 0 (mapped from SLURM_GPUS_ON_NODE=1)"
    else
        export CUDA_VISIBLE_DEVICES=0
        echo "‚úÖ CUDA_VISIBLE_DEVICES set to: 0 (default for any GPU allocation)"
    fi
else
    echo "‚ö†Ô∏è  SLURM_GPUS_ON_NODE not set, using default GPU 0"
    export CUDA_VISIBLE_DEVICES=0
fi

# CUDA Ïª®ÌÖçÏä§Ìä∏ Ï¥àÍ∏∞Ìôî (segmentation fault Î∞©ÏßÄ)
export CUDA_LAUNCH_BLOCKING=1
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True

# PyTorch CUDA 12.4 ÎùºÏù¥Î∏åÎü¨Î¶¨ ÏÇ¨Ïö©
export LD_LIBRARY_PATH="$HOME/anaconda3/envs/tlqkf/lib/python3.12/site-packages/torch/lib:$LD_LIBRARY_PATH"
export CUDA_HOME="$HOME/anaconda3/envs/tlqkf/lib/python3.12/site-packages/torch/lib"

# PyTorch CUDA ÏÑ§Ï†ï
export TORCH_CUDA_ARCH_LIST="8.6"
export CUDA_LAUNCH_BLOCKING=1
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True

# CUDA ÌôòÍ≤ΩÎ≥ÄÏàò (PyTorch ÎÇ¥Ïû• CUDA 12.4 ÏÇ¨Ïö©)
export CUDA_PATH="$HOME/anaconda3/envs/tlqkf/lib/python3.12/site-packages/torch/lib"
export CUDA_ROOT="$HOME/anaconda3/envs/tlqkf/lib/python3.12/site-packages/torch/lib"
echo ""

# GPU Ï†ïÎ≥¥ Ï∂úÎ†•
echo "üîç GPU Information:"
nvidia-smi --query-gpu=name,memory.total,memory.free,utilization.gpu --format=csv,noheader,nounits
echo ""

# PythonÏúºÎ°ú GPU ÌôïÏù∏ (ÏÉÅÏÑ∏ ÏßÑÎã®)
echo "üîç Python GPU Check:"
python -c "
try:
    import torch
    print(f'PyTorch version: {torch.__version__}')
    print(f'CUDA available: {torch.cuda.is_available()}')
    print(f'CUDA version: {torch.version.cuda if torch.cuda.is_available() else \"N/A\"}')
    print(f'CUDA_HOME: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"Not available\"}')
    
    # ÌôòÍ≤ΩÎ≥ÄÏàò ÌôïÏù∏
    import os
    print(f'CUDA_HOME env: {os.environ.get(\"CUDA_HOME\", \"Not set\")}')
    print(f'LD_LIBRARY_PATH: {os.environ.get(\"LD_LIBRARY_PATH\", \"Not set\")[:100]}...')
    print(f'CUDA_VISIBLE_DEVICES: {os.environ.get(\"CUDA_VISIBLE_DEVICES\", \"Not set\")}')
    
    if torch.cuda.is_available():
        print(f'CUDA device count: {torch.cuda.device_count()}')
        print(f'Current device: {torch.cuda.current_device()}')
        print(f'Device name: {torch.cuda.get_device_name(0)}')
        print('‚úÖ CUDA is working!')
    else:
        print('‚ùå CUDA not available in PyTorch')
        print('Debugging info:')
        print('1. PyTorch CUDA support: Check if PyTorch was compiled with CUDA')
        print('2. CUDA libraries: Check if CUDA libraries are in LD_LIBRARY_PATH')
        print('3. GPU memory: Check if GPU has enough memory')
        print('4. CUDA_VISIBLE_DEVICES: Check if GPU is visible to PyTorch')
except Exception as e:
    print(f'‚ö†Ô∏è  Python torch import failed: {e}')
    import traceback
    traceback.print_exc()
"
echo ""

echo "‚úÖ GPU allocation test completed!"
echo "Time: $(date)"
