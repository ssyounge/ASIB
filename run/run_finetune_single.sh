#!/usr/bin/env bash
#SBATCH --job-name=ft_single_teacher
#SBATCH --partition=base_suma_rtx3090
#SBATCH --gres=gpu:1
#SBATCH --cpus-per-task=4
#SBATCH --mem=16G
#SBATCH --time=10:00:00
#SBATCH --output=experiments/finetune/logs/ft_%j.log
#SBATCH --error=experiments/finetune/logs/ft_%j.err
# ---------------------------------------------------------
# Fine-tune single teacher checkpoint
# ê°œë³„ teacher ëª¨ë¸ í•˜ë‚˜ë§Œ finetune
# ---------------------------------------------------------
set -euo pipefail

# Python í™˜ê²½ ì„¤ì •
echo "ğŸ”§ Setting up Python environment..."
set +u
if [ -f "$HOME/anaconda3/etc/profile.d/conda.sh" ]; then
  source "$HOME/anaconda3/etc/profile.d/conda.sh"
fi
conda activate tlqkf || export PATH="$HOME/anaconda3/envs/tlqkf/bin:$PATH"
set -u
echo "âœ… Python environment setup completed"
echo ""

# 1) ë¦¬í¬ ìµœìƒìœ„ë¡œ ì´ë™
ROOT="$(git rev-parse --show-toplevel)"
cd "$ROOT"

# 2) PYTHONPATH ì¶”ê°€ (ë‚´ë¶€ ëª¨ë“ˆ import ìš©)
export PYTHONPATH="${ROOT}:${PYTHONPATH:-}"
export PATH="$HOME/.local/bin:$PATH"

# 3) GPU í• ë‹¹ í™•ì¸ ë° ì„¤ì •
echo "ğŸ” Checking GPU allocation..."
if [ -n "$SLURM_GPUS_ON_NODE" ]; then
    # GPU ì¸ë±ìŠ¤ë¥¼ 0ë¶€í„° ì‹œì‘í•˜ë„ë¡ ì¡°ì •
    if [ "$SLURM_GPUS_ON_NODE" = "1" ]; then
        export CUDA_VISIBLE_DEVICES=0
        echo "âœ… CUDA_VISIBLE_DEVICES set to: 0 (mapped from SLURM_GPUS_ON_NODE=1)"
    else
        export CUDA_VISIBLE_DEVICES=0
        echo "âœ… CUDA_VISIBLE_DEVICES set to: 0 (default for any GPU allocation)"
    fi
else
    echo "âš ï¸  SLURM_GPUS_ON_NODE not set, using default GPU 0"
    export CUDA_VISIBLE_DEVICES=0
fi

# CUDA ì»¨í…ìŠ¤íŠ¸ ì´ˆê¸°í™” (segmentation fault ë°©ì§€)
export CUDA_LAUNCH_BLOCKING=1
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True

# PyTorch CUDA 12.4 ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©
unset LD_LIBRARY_PATH || true
export CUDA_HOME="$HOME/anaconda3/envs/tlqkf/lib/python3.12/site-packages/torch/lib"

# PyTorch CUDA ì„¤ì •
export TORCH_CUDA_ARCH_LIST="8.6"

# CUDA í™˜ê²½ë³€ìˆ˜ (PyTorch ë‚´ì¥ CUDA 12.4 ì‚¬ìš©)
export CUDA_PATH="$HOME/anaconda3/envs/tlqkf/lib/python3.12/site-packages/torch/lib"
export CUDA_ROOT="$HOME/anaconda3/envs/tlqkf/lib/python3.12/site-packages/torch/lib"

# GPU ì •ë³´ ì¶œë ¥
echo "ğŸ” GPU Information:"
nvidia-smi --query-gpu=name,memory.total,memory.free --format=csv,noheader,nounits



# 4) ì¸ì: <yaml basename> (default=convnext_s_cifar100)
CFG_NAME="${1:-convnext_s_cifar100}"
# convnext_s_cifar100, convnext_s_imagenet32, convnext_l_cifar100, convnext_l_imagenet32, efficientnet_l2_cifar100, efficientnet_l2_imagenet32, resnet152_cifar100, resnet152_imagenet32
shift || true         # ë‚˜ë¨¸ì§€ ì¸ì â†’ Hydra override

# 5) ì‹¤í–‰
# Hydra-safe ì¸ìë§Œ ì „ë‹¬
PASSTHRU_ARGS=()
for a in "$@"; do
  if [[ "$a" == -* || "$a" == *=* || "$a" == +*=* ]]; then
    PASSTHRU_ARGS+=("$a")
  fi
done
python scripts/training/fine_tuning.py -cn="finetune/$CFG_NAME" \
    "${PASSTHRU_ARGS[@]}"

echo "[run_finetune_single.sh] âœ… finished â€“ $CFG_NAME"
