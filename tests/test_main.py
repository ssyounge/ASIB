#!/usr/bin/env python3
"""
main.py ÌõàÎ†® Ïã§Ìñâ ÌÖåÏä§Ìä∏
"""

import torch
import logging
from omegaconf import DictConfig, OmegaConf

# Î°úÍπÖ ÏÑ§Ï†ï
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def test_main_training():
    """main.py ÌõàÎ†® Ïã§Ìñâ ÌÖåÏä§Ìä∏"""
    logger.info("üöÄ Testing main.py training execution...")
    
    # Í∞ÑÎã®Ìïú ÏÑ§Ï†ï ÏÉùÏÑ±
    config = {
        "experiment": {
            "dataset": {
                "dataset": {
                    "name": "cifar100",
                    "root": "./data",
                    "small_input": True,
                    "data_aug": 1
                },
                "batch_size": 4,
                "num_workers": 0
            },
            "method": {
                "method": {
                    "name": "asib",
                    "ce_alpha": 0.3,
                    "kd_alpha": 0.0,
                    "kd_ens_alpha": 0.7
                }
            },
            "schedule": {
                "type": "cosine",
                "lr_warmup_epochs": 5,
                "min_lr": 1e-05
            },
            "device": "cuda",
            "seed": 42,
            "small_input": True,
            "batch_size": 4,
            "use_partial_freeze": False,
            "use_amp": True,
            "amp_dtype": "float16",
            "kd_ens_alpha": 0.5,
            "hybrid_beta": 0.05,
            "mixup_alpha": 0.0,
            "cutmix_alpha_distill": 0.0,
            "use_disagree_weight": False,
            "disagree_mode": "both_wrong",
            "disagree_lambda_high": 1.0,
            "disagree_lambda_low": 1.0,
            "feat_kd_alpha": 0.0,
            "feat_kd_key": "feat_2d",
            "feat_kd_norm": "none",
            "rkd_loss_weight": 0.0,
            "rkd_gamma": 2.0,
            "use_ib": False,
            "ib_beta": 0.0,
            "ib_beta_warmup_epochs": 0,
            "ib_mbm_out_dim": 2048,
            "ib_mbm_n_head": 8,
            "ib_mbm_dropout": 0.0,
            "synergy_head_dropout": 0.0,
            "ib_mbm_learnable_q": False,
            "mbm_reg_lambda": 0.0,
            "use_cccp": False,
            "tau": 4.0,
            "reg_lambda": 0.0,
            "grad_clip_norm": 1.0,
            "adam_beta1": 0.9,
            "adam_beta2": 0.999,
            "disable_flops": True,
            "student_epochs_per_stage": 15,
            "teacher1": {
                "model": {
                    "teacher": {
                        "name": "convnext_s",
                        "pretrained": True
                    }
                }
            },
            "teacher2": {
                "model": {
                    "teacher": {
                        "name": "resnet152",
                        "pretrained": True
                    }
                }
            },
            "model": {
                "student": {
                    "model": {
                        "student": {
                            "name": "resnet50_scratch",
                            "pretrained": False,
                            "use_adapter": True
                        }
                    }
                }
            },
            "teacher1_ckpt": "checkpoints/teachers/convnext_s_cifar32.pth",
"teacher2_ckpt": "checkpoints/teachers/resnet152_cifar32.pth",
                            "results_dir": ".",
            "exp_id": "test",
            "optimizer": "sgd",
            "teacher_lr": 0.0,
            "teacher_weight_decay": 0.0,
            "student_lr": 0.1,
            "student_weight_decay": 0.0003,
            "momentum": 0.9,
            "nesterov": True,
            "num_stages": 1,
            "teacher_adapt_epochs": 0,
            "student_freeze_schedule": [0],
            "student_epochs_schedule": [1],
            "teacher1_freeze_level": -1,
            "teacher2_freeze_level": -1,
            "student_freeze_bn": False,
            "teacher1_freeze_bn": True,
            "teacher2_freeze_bn": True,
            "use_distillation_adapter": True,
            "distill_out_dim": 512,
            "ib_mbm_query_dim": 2048,
            "kd_alpha": 0.5,
            "cccp_nt": 1,
            "cccp_ns": 1
        }
    }
    
    # DictConfigÎ°ú Î≥ÄÌôò
    cfg = OmegaConf.create(config)
    
    logger.info("‚úÖ Configuration created successfully")
    logger.info(f"Device: {cfg.experiment.device}")
    logger.info(f"Batch size: {cfg.experiment.batch_size}")
    logger.info(f"Student epochs: {cfg.experiment.student_epochs_schedule}")
    
    # CUDA ÏÇ¨Ïö© Í∞ÄÎä• Ïó¨Î∂Ä ÌôïÏù∏
    if torch.cuda.is_available():
        logger.info(f"CUDA available: {torch.cuda.get_device_name()}")
        logger.info(f"CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
    else:
        logger.warning("CUDA not available!")
    
    logger.info("‚úÖ Test completed!")

if __name__ == "__main__":
    test_main_training() 