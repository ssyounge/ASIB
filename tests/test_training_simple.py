#!/usr/bin/env python3
"""
ê°„ë‹¨í•œ í›ˆë ¨ í…ŒìŠ¤íŠ¸
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def test_simple_training():
    """ê°„ë‹¨í•œ í›ˆë ¨ í…ŒìŠ¤íŠ¸"""
    logger.info("ğŸš€ Starting simple training test...")
    
    # CUDA í™•ì¸
    if torch.cuda.is_available():
        device_name = torch.cuda.get_device_name()
        device_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
        logger.info(f"Device: {device_name} ({device_memory:.1f} GB)")
    else:
        logger.error("CUDA not available!")
        return
    
    # ê°„ë‹¨í•œ ëª¨ë¸ ìƒì„±
    model = nn.Linear(10, 1).cuda()
    optimizer = optim.SGD(model.parameters(), lr=0.01)
    criterion = nn.MSELoss()
    
    # ë”ë¯¸ ë°ì´í„° ìƒì„±
    x = torch.randn(100, 10).cuda()
    y = torch.randn(100, 1).cuda()
    dataset = TensorDataset(x, y)
    dataloader = DataLoader(dataset, batch_size=10)
    
    # í›ˆë ¨ ë£¨í”„
    model.train()
    for epoch in range(3):
        total_loss = 0
        for batch_idx, (data, target) in enumerate(dataloader):
            optimizer.zero_grad()
            output = model(data)
            loss = criterion(output, target)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
            
            if batch_idx % 5 == 0:
                logger.info(f"Epoch {epoch+1}, Batch {batch_idx}, Loss: {loss.item():.4f}")
        
        avg_loss = total_loss / len(dataloader)
        logger.info(f"Epoch {epoch+1} completed. Average Loss: {avg_loss:.4f}")
    
    logger.info("âœ… Simple training test completed!")
    
    # ë©”ëª¨ë¦¬ ì •ë¦¬
    del model, optimizer, criterion, dataloader, dataset, x, y
    torch.cuda.empty_cache()
    
    logger.info("âœ… Memory cleanup completed!")

if __name__ == "__main__":
    test_simple_training() 