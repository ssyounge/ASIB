# ASIB (Adaptive Synergy Information-Bottleneck) Method Guide

## ğŸ“‹ ëª©ì°¨
1. [ê°œìš”](#ê°œìš”)
2. [í•µì‹¬ ì•„ì´ë””ì–´](#í•µì‹¬-ì•„ì´ë””ì–´)
3. [ì•„í‚¤í…ì²˜](#ì•„í‚¤í…ì²˜)
4. [í•˜ì´í¼íŒŒë¼ë¯¸í„°](#í•˜ì´í¼íŒŒë¼ë¯¸í„°)
5. [êµ¬í˜„ ì„¸ë¶€ì‚¬í•­](#êµ¬í˜„-ì„¸ë¶€ì‚¬í•­)
6. [ì‹¤í—˜ ì„¤ì •](#ì‹¤í—˜-ì„¤ì •)
7. [ì‚¬ìš©ë²•](#ì‚¬ìš©ë²•)
8. [ì„±ëŠ¥ ìµœì í™”](#ì„±ëŠ¥-ìµœì í™”)

---

## ğŸ¯ ê°œìš”

**ASIB (Adaptive Synergy Information-Bottleneck)**ëŠ” ì§€ì‹ ì¦ë¥˜(Knowledge Distillation)ì™€ ì •ë³´ ë³‘ëª©(Information Bottleneck)ì„ ê²°í•©í•œ í˜ì‹ ì ì¸ ë°©ë²•ë¡ ì…ë‹ˆë‹¤.

### ì£¼ìš” íŠ¹ì§•
- **2ëª…ì˜ Teacher ëª¨ë¸**ê³¼ **1ëª…ì˜ Student ëª¨ë¸** êµ¬ì¡°
- **IB-MBM (Information-Bottleneck Manifold Bridging Module)**ì„ í†µí•œ ì •ë³´ ì••ì¶•
- **Multi-stage í•™ìŠµ** ê³¼ì • (Teacher Adaptive â†’ Student Distillation)
- **ì•ˆì •ì„±-ê°€ì†Œì„± ê· í˜•** ìµœì í™”

---

## ğŸ§  í•µì‹¬ ì•„ì´ë””ì–´

### 1. ì •ë³´ ë³‘ëª© (Information Bottleneck)
```
Input â†’ Encoder â†’ Compressed Representation â†’ Decoder â†’ Output
                â†‘
            IB Module
```

- **ëª©ì **: ë¶ˆí•„ìš”í•œ ì •ë³´ë¥¼ ì œê±°í•˜ê³  í•µì‹¬ ì •ë³´ë§Œ ì „ë‹¬
- **ì¥ì **: ëª¨ë¸ ìš©ëŸ‰ ì ˆì•½, ê³¼ì í•© ë°©ì§€, ì¼ë°˜í™” ì„±ëŠ¥ í–¥ìƒ

### 2. ì‹œë„ˆì§€ íš¨ê³¼ (Synergy)
- **Teacher1**ê³¼ **Teacher2**ì˜ ì§€ì‹ì„ **MBM**ì„ í†µí•´ ê²°í•©
- **Student**ëŠ” ì••ì¶•ëœ ì‹œë„ˆì§€ ì§€ì‹ì„ í•™ìŠµ

### 3. ì ì‘ì  í•™ìŠµ (Adaptive Learning)
- **Stage A**: Teacher ëª¨ë¸ë“¤ ì ì‘ì  ì—…ë°ì´íŠ¸
- **Stage B**: Student ì§€ì‹ ì¦ë¥˜
- ë°˜ë³µì„ í†µí•œ ì ì§„ì  ì„±ëŠ¥ í–¥ìƒ

---

## ğŸ—ï¸ ì•„í‚¤í…ì²˜

### ì „ì²´ êµ¬ì¡° ë° ë°ì´í„° í”Œë¡œìš°

```
Input Image
    â”‚
    â”œâ”€â”€ Teacher1 â”€â”€â”
    â”‚              â”‚
    â”œâ”€â”€ Teacher2 â”€â”€â”¼â”€â”€ MBM â”€â”€ Synergy Head â”€â”€ Student â”€â”€ Output
    â”‚              â”‚
    â””â”€â”€ Student â”€â”€â”€â”˜
```

### ìƒì„¸ ì•„í‚¤í…ì²˜ êµ¬ì„±

#### 1. ASIBDistiller (ë©”ì¸ ë””ìŠ¤í‹¸ëŸ¬)
```python
class ASIBDistiller(nn.Module):
    """
    Adaptive Synergy Information-Bottleneck Distiller
    
    í•µì‹¬ êµ¬ì„±ìš”ì†Œ:
    - teacher1, teacher2: ë‘ ê°œì˜ êµì‚¬ ëª¨ë¸
    - student: í•™ìŠµí•  í•™ìƒ ëª¨ë¸
    - mbm: Manifold Bridging Module (ì •ë³´ ë³‘ëª© ê¸°ë°˜)
    - synergy_head: ì‹œë„ˆì§€ íš¨ê³¼ë¥¼ ìƒì„±í•˜ëŠ” í—¤ë“œ
    """
    
    def __init__(
        self,
        teacher1,                    # ì²« ë²ˆì§¸ êµì‚¬ ëª¨ë¸ (ì˜ˆ: ConvNeXt-L)
        teacher2,                    # ë‘ ë²ˆì§¸ êµì‚¬ ëª¨ë¸ (ì˜ˆ: ResNet-152)
        student,                     # í•™ìƒ ëª¨ë¸ (ì˜ˆ: ResNet-50)
        mbm,                         # Manifold Bridging Module
        synergy_head,                # ì‹œë„ˆì§€ í—¤ë“œ
        alpha=0.5,                   # CE vs KL ë¹„ìœ¨ (0.5 = 50:50)
        synergy_ce_alpha=0.3,        # ì‹œë„ˆì§€ CE ë¹„ì¤‘
        temperature=4.0,             # ì§€ì‹ ì¦ë¥˜ ì˜¨ë„ (ë†’ì„ìˆ˜ë¡ ë¶€ë“œëŸ¬ìš´ í™•ë¥  ë¶„í¬)
        reg_lambda=1e-4,             # ì •ê·œí™” ê°€ì¤‘ì¹˜
        mbm_reg_lambda=1e-4,         # MBM ì •ê·œí™” ê°€ì¤‘ì¹˜
        num_stages=2,                # í•™ìŠµ ìŠ¤í…Œì´ì§€ ìˆ˜
        device="cuda",               # ë””ë°”ì´ìŠ¤
        config=None                  # ì¶”ê°€ ì„¤ì •
    ):
        super().__init__()
        
        # ëª¨ë¸ë“¤
        self.teacher1 = teacher1
        self.teacher2 = teacher2
        self.student = student
        self.mbm = mbm
        self.synergy_head = synergy_head
        
        # í•˜ì´í¼íŒŒë¼ë¯¸í„°
        cfg = config or {}
        self.alpha = cfg.get("ce_alpha", alpha)
        self.synergy_ce_alpha = cfg.get("synergy_ce_alpha", synergy_ce_alpha)
        self.kd_warmup_stage = cfg.get("teacher_adapt_kd_warmup", 2)
        self.T = cfg.get("tau_start", temperature)
        self.reg_lambda = cfg.get("reg_lambda", reg_lambda)
        self.mbm_reg_lambda = cfg.get("mbm_reg_lambda", mbm_reg_lambda)
        self.num_stages = cfg.get("num_stages", num_stages)
        self.device = device
        self.config = config if config is not None else {}
        
        # ì†ì‹¤ í•¨ìˆ˜
        self.ce_loss_fn = nn.CrossEntropyLoss()
```

#### 2. IB-MBM (Information-Bottleneck Manifold Bridging Module)
```python
class IB_MBM(nn.Module):
    """
    Information-Bottleneck Manifold Bridging Module
    
    ê¸°ëŠ¥:
    1. Teacherë“¤ì˜ íŠ¹ì§•ì„ ì •ë³´ ë³‘ëª©ì„ í†µí•´ ì••ì¶•
    2. ì••ì¶•ëœ ì •ë³´ë¥¼ Studentì—ê²Œ íš¨ìœ¨ì ìœ¼ë¡œ ì „ë‹¬
    3. ë¶ˆí•„ìš”í•œ ì •ë³´ ì œê±°ë¡œ ëª¨ë¸ ìš©ëŸ‰ ì ˆì•½
    """
    
    def __init__(
        self,
        query_dim,                   # ì¿¼ë¦¬ ì°¨ì› (Student íŠ¹ì§• ì°¨ì›)
        key_dim,                     # í‚¤ ì°¨ì› (Teacher íŠ¹ì§• ì°¨ì›)
        out_dim,                     # ì¶œë ¥ ì°¨ì› (ì••ì¶•ëœ íŠ¹ì§• ì°¨ì›)
        n_head=8,                    # ë©€í‹°í—¤ë“œ ì–´í…ì…˜ í—¤ë“œ ìˆ˜
        dropout=0.0,                 # ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨
        learnable_q=False,           # í•™ìŠµ ê°€ëŠ¥í•œ ì¿¼ë¦¬ ì—¬ë¶€
        mbm_reg_lambda=0.0           # ì •ê·œí™” ê°€ì¤‘ì¹˜
    ):
        super().__init__()
        
        # ì°¨ì› ì„¤ì •
        self.query_dim = query_dim
        self.key_dim = key_dim
        self.out_dim = out_dim
        self.n_head = n_head
        self.dropout = dropout
        
        # ì¿¼ë¦¬ ìƒì„± (Student íŠ¹ì§•ì—ì„œ)
        if learnable_q:
            self.query = nn.Parameter(torch.randn(1, out_dim, query_dim))
        else:
            self.query = None
        
        # í‚¤/ê°’ ë³€í™˜ (Teacher íŠ¹ì§•ì—ì„œ)
        self.key_proj = nn.Linear(key_dim, out_dim)
        self.value_proj = nn.Linear(key_dim, out_dim)
        
        # ë©€í‹°í—¤ë“œ ì–´í…ì…˜
        self.attention = nn.MultiheadAttention(
            embed_dim=out_dim,
            num_heads=n_head,
            dropout=dropout,
            batch_first=True
        )
        
        # ì¶œë ¥ ë³€í™˜
        self.output_proj = nn.Linear(out_dim, out_dim)
        self.layer_norm = nn.LayerNorm(out_dim)
        
        # ì •ê·œí™”
        self.mbm_reg_lambda = mbm_reg_lambda
    
    def forward(self, query_feat, key_feat):
        """
        Args:
            query_feat: Student íŠ¹ì§• (B, query_dim)
            key_feat: Teacher íŠ¹ì§•ë“¤ (B, num_teachers, key_dim)
        
        Returns:
            compressed_feat: ì••ì¶•ëœ íŠ¹ì§• (B, out_dim)
        """
        batch_size = query_feat.size(0)
        
        # ì¿¼ë¦¬ ìƒì„±
        if self.query is not None:
            q = self.query.expand(batch_size, -1, -1)  # (B, out_dim, query_dim)
        else:
            q = query_feat.unsqueeze(1)  # (B, 1, query_dim)
        
        # í‚¤/ê°’ ë³€í™˜
        k = self.key_proj(key_feat)  # (B, num_teachers, out_dim)
        v = self.value_proj(key_feat)  # (B, num_teachers, out_dim)
        
        # ì–´í…ì…˜ ê³„ì‚°
        attn_output, attn_weights = self.attention(q, k, v)
        
        # ì¶œë ¥ ë³€í™˜
        output = self.output_proj(attn_output.squeeze(1))
        output = self.layer_norm(output)
        
        return output, attn_weights
```

#### 3. Synergy Head (ì‹œë„ˆì§€ í—¤ë“œ)
```python
class SynergyHead(nn.Module):
    """
    ì‹œë„ˆì§€ íš¨ê³¼ë¥¼ ìƒì„±í•˜ëŠ” í—¤ë“œ
    
    ê¸°ëŠ¥:
    1. MBMì—ì„œ ë‚˜ì˜¨ ì••ì¶•ëœ íŠ¹ì§•ì„ ë¶„ë¥˜ ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜
    2. Teacherë“¤ì˜ ì‹œë„ˆì§€ íš¨ê³¼ë¥¼ Studentì—ê²Œ ì „ë‹¬
    """
    
    def __init__(
        self,
        input_dim,                   # ì…ë ¥ ì°¨ì› (MBM ì¶œë ¥ ì°¨ì›)
        num_classes,                 # í´ë˜ìŠ¤ ìˆ˜
        dropout=0.0,                 # ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨
        hidden_dim=None              # ìˆ¨ê²¨ì§„ ì°¨ì›
    ):
        super().__init__()
        
        if hidden_dim is None:
            hidden_dim = input_dim // 2
        
        self.synergy_classifier = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, num_classes)
        )
    
    def forward(self, compressed_feat):
        """
        Args:
            compressed_feat: MBMì—ì„œ ì••ì¶•ëœ íŠ¹ì§• (B, input_dim)
        
        Returns:
            synergy_logit: ì‹œë„ˆì§€ ë¡œì§“ (B, num_classes)
        """
        return self.synergy_classifier(compressed_feat)
```

#### 4. ASIB-CL (Continual Learning ë²„ì „)
```python
class ASIB_CL(BaseLearner):
    """
    ASIB-CL: Information Bottleneck ê¸°ë°˜ Class-Incremental Learning
    
    í•µì‹¬ ì•„ì´ë””ì–´:
    1. ì´ì „ ëª¨ë¸(M_{T-1})ì„ êµì‚¬ë¡œ ì‚¬ìš©
    2. IB ê¸°ë°˜ ì§€ì‹ ì¦ë¥˜ë¡œ ì•ˆì •ì„±-ê°€ì†Œì„± ìµœì í™”
    3. ìµœì†Œ ì¶©ë¶„ ì •ë³´ë§Œ ì „ë‹¬í•˜ì—¬ ëª¨ë¸ ìš©ëŸ‰ í™•ë³´
    """
    
    def __init__(self, args):
        super().__init__(args)
        
        # ë„¤íŠ¸ì›Œí¬ ì´ˆê¸°í™”
        self._network = IncrementalNet(args, False)
        self._class_means = None
        self._old_network = None
        
        # IB ê´€ë ¨ í•˜ì´í¼íŒŒë¼ë¯¸í„°
        self._ib_beta = args.get('ib_beta', 0.1)      # IB ì••ì¶• ê°•ë„
        self.lambda_D = args.get('lambda_D', 1.0)     # ì¦ë¥˜ ì†ì‹¤ ê°€ì¤‘ì¹˜
        self.lambda_IB = args.get('lambda_IB', 1.0)   # IB ëª¨ë“ˆ ì†ì‹¤ ê°€ì¤‘ì¹˜
        
        # IB ëª¨ë“ˆë“¤
        self._ib_encoder = None  # VIB ì¸ì½”ë”
        self._ib_decoder = None  # VIB ë””ì½”ë”
        
        # ë°ì´í„° ë©”ëª¨ë¦¬ (Continual Learningìš©)
        self._data_memory = np.array([])
        self._targets_memory = np.array([])
        
        # ë„¤íŠ¸ì›Œí¬ ì´ˆê¸°í™”
        self._network.update_fc(10)  # ê¸°ë³¸ í´ë˜ìŠ¤ ìˆ˜ë¡œ ì´ˆê¸°í™”
```

### ë°ì´í„° í”Œë¡œìš° ìƒì„¸ ì„¤ëª…

#### 1. Forward Pass (ìˆœì „íŒŒ)
```
1. Input Image â†’ Teacher1, Teacher2, Student
2. Teacher íŠ¹ì§•ë“¤ â†’ MBM (ì •ë³´ ì••ì¶•)
3. MBM ì¶œë ¥ â†’ Synergy Head (ì‹œë„ˆì§€ ìƒì„±)
4. Synergy + Student â†’ ìµœì¢… ì¶œë ¥
```

#### 2. Backward Pass (ì—­ì „íŒŒ)
```
1. ì†ì‹¤ ê³„ì‚° (CE + KL + IB)
2. Student ì—…ë°ì´íŠ¸
3. MBM ì—…ë°ì´íŠ¸ (ì„ íƒì )
4. Synergy Head ì—…ë°ì´íŠ¸
```

#### 3. Multi-Stage í•™ìŠµ
```
Stage 1: Teacher Adaptive Update
â”œâ”€â”€ Teacher1, Teacher2 íŠ¹ì§• ì¶”ì¶œ
â”œâ”€â”€ MBMì„ í†µí•œ ì •ë³´ ì••ì¶•
â”œâ”€â”€ Synergy Head í•™ìŠµ
â””â”€â”€ CCCP ì†ì‹¤ë¡œ ì•ˆì •ì„± í™•ë³´

Stage 2: Student Distillation
â”œâ”€â”€ Student íŠ¹ì§• ì¶”ì¶œ
â”œâ”€â”€ MBMì„ í†µí•œ ì‹œë„ˆì§€ ìƒì„±
â”œâ”€â”€ KL Divergence ê³„ì‚°
â””â”€â”€ Student ì—…ë°ì´íŠ¸
```

---

## âš™ï¸ í•˜ì´í¼íŒŒë¼ë¯¸í„°

### ğŸ“Š í•˜ì´í¼íŒŒë¼ë¯¸í„° ë¶„ë¥˜ ë° ìƒì„¸ ì„¤ëª…

#### 1. ê¸°ë³¸ ì§€ì‹ ì¦ë¥˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°

| íŒŒë¼ë¯¸í„° | ê¸°ë³¸ê°’ | ë²”ìœ„ | ì„¤ëª… | ì˜í–¥ |
|---------|--------|------|------|------|
| `ce_alpha` | 0.3 | [0.0, 1.0] | Student CE vs KL ë¹„ìœ¨ | ë†’ì„ìˆ˜ë¡ ì›ë³¸ íƒœìŠ¤í¬ì— ì§‘ì¤‘ |
| `kd_alpha` | 0.0 | [0.0, 1.0] | ê¸°ë³¸ KD ê°€ì¤‘ì¹˜ | ê¸°ë³¸ ì§€ì‹ ì¦ë¥˜ ê°•ë„ |
| `kd_ens_alpha` | 0.7 | [0.0, 1.0] | ì•™ìƒë¸” KD ê°€ì¤‘ì¹˜ | Teacher ì•™ìƒë¸” íš¨ê³¼ ê°•ë„ |
| `temperature` | 4.0 | [1.0, 10.0] | ì§€ì‹ ì¦ë¥˜ ì˜¨ë„ | ë†’ì„ìˆ˜ë¡ ë¶€ë“œëŸ¬ìš´ í™•ë¥  ë¶„í¬ |
| `num_stages` | 2 | [1, 5] | í•™ìŠµ ìŠ¤í…Œì´ì§€ ìˆ˜ | Teacher-Student ë°˜ë³µ íšŸìˆ˜ |

**ìƒì„¸ ì„¤ëª…:**
- **`ce_alpha`**: Cross-Entropyì™€ KL Divergenceì˜ ê· í˜•ì„ ì¡°ì ˆ
  - `0.0`: ìˆœìˆ˜ ì§€ì‹ ì¦ë¥˜ (Teacherë§Œ í•™ìŠµ)
  - `1.0`: ìˆœìˆ˜ ë¶„ë¥˜ í•™ìŠµ (Studentë§Œ í•™ìŠµ)
  - `0.3`: 30% CE + 70% KD (ê¶Œì¥ê°’)

- **`temperature`**: í™•ë¥  ë¶„í¬ì˜ ë¶€ë“œëŸ¬ì›€ ì¡°ì ˆ
  - ë‚®ì€ ê°’: í™•ì‹¤í•œ ì˜ˆì¸¡ (í•˜ë“œ íƒ€ê²Ÿ)
  - ë†’ì€ ê°’: ë¶ˆí™•ì‹¤í•œ ì˜ˆì¸¡ (ì†Œí”„íŠ¸ íƒ€ê²Ÿ)
  - `4.0`: ì¼ë°˜ì ìœ¼ë¡œ ì¢‹ì€ ì„±ëŠ¥

#### 2. ì •ë³´ ë³‘ëª© (IB) ê´€ë ¨ í•˜ì´í¼íŒŒë¼ë¯¸í„°

| íŒŒë¼ë¯¸í„° | ê¸°ë³¸ê°’ | ë²”ìœ„ | ì„¤ëª… | ì˜í–¥ |
|---------|--------|------|------|------|
| `use_ib` | false | {true, false} | IB ì‚¬ìš© ì—¬ë¶€ | IB ëª¨ë“ˆ í™œì„±í™” |
| `ib_beta` | 0.01 | [0.001, 1.0] | IB ì••ì¶• ê°•ë„ | ë†’ì„ìˆ˜ë¡ ê°•í•œ ì••ì¶• |
| `ib_beta_warmup_epochs` | 0 | [0, 10] | IB ë² íƒ€ ì›Œë°ì—… ì—í¬í¬ | ì ì§„ì  ì••ì¶• ê°•í™” |
| `mbm_out_dim` | 512 | [256, 2048] | MBM ì¶œë ¥ ì°¨ì› | ì••ì¶•ëœ íŠ¹ì§• ì°¨ì› |
| `mbm_n_head` | 1 | [1, 16] | MBM ì–´í…ì…˜ í—¤ë“œ ìˆ˜ | ë©€í‹°í—¤ë“œ ì–´í…ì…˜ |
| `mbm_dropout` | 0.0 | [0.0, 0.5] | MBM ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨ | ê³¼ì í•© ë°©ì§€ |
| `mbm_learnable_q` | false | {true, false} | í•™ìŠµ ê°€ëŠ¥í•œ ì¿¼ë¦¬ | ì¿¼ë¦¬ ìµœì í™” |
| `mbm_reg_lambda` | 0.0 | [0.0, 1.0] | MBM ì •ê·œí™” ê°€ì¤‘ì¹˜ | MBM ì •ê·œí™” |

**ìƒì„¸ ì„¤ëª…:**
- **`ib_beta`**: ì •ë³´ ë³‘ëª©ì˜ ì••ì¶• ê°•ë„
  - `0.001`: ë§¤ìš° ì•½í•œ ì••ì¶• (ê±°ì˜ ì›ë³¸ ìœ ì§€)
  - `0.01`: ì•½í•œ ì••ì¶• (ê¶Œì¥ ì‹œì‘ê°’)
  - `0.1`: ì¤‘ê°„ ì••ì¶• (ê· í˜•ì )
  - `1.0`: ê°•í•œ ì••ì¶• (ë§ì€ ì •ë³´ ì†ì‹¤)

- **`mbm_out_dim`**: ì••ì¶•ëœ íŠ¹ì§•ì˜ ì°¨ì›
  - Teacher íŠ¹ì§• ì°¨ì›ë³´ë‹¤ ì‘ì•„ì•¼ í•¨
  - ë„ˆë¬´ ì‘ìœ¼ë©´ ì •ë³´ ì†ì‹¤
  - ë„ˆë¬´ í¬ë©´ ì••ì¶• íš¨ê³¼ ì—†ìŒ

#### 3. CCCP (Concave-Convex Procedure) ê´€ë ¨ í•˜ì´í¼íŒŒë¼ë¯¸í„°

| íŒŒë¼ë¯¸í„° | ê¸°ë³¸ê°’ | ë²”ìœ„ | ì„¤ëª… | ì˜í–¥ |
|---------|--------|------|------|------|
| `use_cccp` | true | {true, false} | CCCP ì‚¬ìš© ì—¬ë¶€ | Teacher ì•ˆì •ì„± |
| `tau` | 4.0 | [1.0, 10.0] | CCCP ì˜¨ë„ íŒŒë¼ë¯¸í„° | Teacher í•™ìŠµ ì•ˆì •ì„± |

**ìƒì„¸ ì„¤ëª…:**
- **`use_cccp`**: Teacher ëª¨ë¸ì˜ ì•ˆì •ì„±ì„ ìœ„í•œ CCCP ì‚¬ìš©
- **`tau`**: CCCPì˜ ì˜¨ë„ íŒŒë¼ë¯¸í„°ë¡œ Teacher í•™ìŠµì˜ ë¶€ë“œëŸ¬ì›€ ì¡°ì ˆ

#### 4. í•™ìŠµ ê´€ë ¨ í•˜ì´í¼íŒŒë¼ë¯¸í„°

| íŒŒë¼ë¯¸í„° | ê¸°ë³¸ê°’ | ë²”ìœ„ | ì„¤ëª… | ì˜í–¥ |
|---------|--------|------|------|------|
| `teacher_lr` | 0.0002 | [1e-5, 1e-2] | Teacher í•™ìŠµë¥  | Teacher ì—…ë°ì´íŠ¸ ì†ë„ |
| `student_lr` | 0.001 | [1e-4, 1e-1] | Student í•™ìŠµë¥  | Student ì—…ë°ì´íŠ¸ ì†ë„ |
| `student_epochs_per_stage` | 15 | [5, 50] | ìŠ¤í…Œì´ì§€ë‹¹ Student ì—í¬í¬ | Student í•™ìŠµ ì‹œê°„ |
| `batch_size` | 128 | [32, 512] | ë°°ì¹˜ í¬ê¸° | ë©”ëª¨ë¦¬ì™€ ì„±ëŠ¥ ê· í˜• |
| `use_amp` | true | {true, false} | Mixed Precision ì‚¬ìš© | í•™ìŠµ ì†ë„ í–¥ìƒ |

#### 5. ì •ê·œí™” ë° ìµœì í™” í•˜ì´í¼íŒŒë¼ë¯¸í„°

| íŒŒë¼ë¯¸í„° | ê¸°ë³¸ê°’ | ë²”ìœ„ | ì„¤ëª… | ì˜í–¥ |
|---------|--------|------|------|------|
| `reg_lambda` | 0.0 | [0.0, 1e-2] | ì¼ë°˜ ì •ê·œí™” ê°€ì¤‘ì¹˜ | ëª¨ë¸ ë³µì¡ë„ ì œì–´ |
| `weight_decay` | 1e-4 | [1e-5, 1e-3] | ê°€ì¤‘ì¹˜ ê°ì‡  | ê³¼ì í•© ë°©ì§€ |
| `grad_clip_norm` | 0.0 | [0.0, 10.0] | ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘ | í•™ìŠµ ì•ˆì •ì„± |
| `adam_beta1` | 0.9 | [0.8, 0.99] | Adam Î²1 | ëª¨ë©˜í…€ ì¡°ì ˆ |
| `adam_beta2` | 0.999 | [0.9, 0.9999] | Adam Î²2 | ì ì‘ì  í•™ìŠµë¥  |

#### 6. ë°ì´í„° ì¦ê°• ê´€ë ¨ í•˜ì´í¼íŒŒë¼ë¯¸í„°

| íŒŒë¼ë¯¸í„° | ê¸°ë³¸ê°’ | ë²”ìœ„ | ì„¤ëª… | ì˜í–¥ |
|---------|--------|------|------|------|
| `mixup_alpha` | 0.0 | [0.0, 1.0] | Mixup ê°•ë„ | ë°ì´í„° ì¦ê°• |
| `cutmix_alpha_distill` | 0.0 | [0.0, 1.0] | CutMix ê°•ë„ | ì§€ì‹ ì¦ë¥˜ìš© ì¦ê°• |
| `data_aug` | true | {true, false} | ë°ì´í„° ì¦ê°• ì‚¬ìš© | ì¼ë°˜í™” ì„±ëŠ¥ |

#### 7. ë¶ˆì¼ì¹˜ ê°€ì¤‘ì¹˜ (Disagreement Weighting) í•˜ì´í¼íŒŒë¼ë¯¸í„°

| íŒŒë¼ë¯¸í„° | ê¸°ë³¸ê°’ | ë²”ìœ„ | ì„¤ëª… | ì˜í–¥ |
|---------|--------|------|------|------|
| `use_disagree_weight` | false | {true, false} | ë¶ˆì¼ì¹˜ ê°€ì¤‘ì¹˜ ì‚¬ìš© | Teacher ë¶ˆì¼ì¹˜ í™œìš© |
| `disagree_mode` | none | {pred, both_wrong, any_wrong, none} | ë¶ˆì¼ì¹˜ ëª¨ë“œ | ê°€ì¤‘ì¹˜ ì ìš© ë°©ì‹ |
| `disagree_lambda_high` | 1.0 | [0.5, 2.0] | ë†’ì€ ë¶ˆì¼ì¹˜ ê°€ì¤‘ì¹˜ | ë¶ˆì¼ì¹˜ ì‹œ ê°•í™” |
| `disagree_lambda_low` | 1.0 | [0.5, 2.0] | ë‚®ì€ ë¶ˆì¼ì¹˜ ê°€ì¤‘ì¹˜ | ì¼ì¹˜ ì‹œ ìœ ì§€ |

### ğŸ¯ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ê°€ì´ë“œ

#### 1. ìš°ì„ ìˆœìœ„ë³„ íŠœë‹ ìˆœì„œ
```
1ìˆœìœ„: ib_beta (0.01 â†’ 0.1 â†’ 0.5)
2ìˆœìœ„: ce_alpha (0.3 â†’ 0.5 â†’ 0.7)
3ìˆœìœ„: temperature (4.0 â†’ 6.0 â†’ 8.0)
4ìˆœìœ„: mbm_out_dim (512 â†’ 1024 â†’ 2048)
5ìˆœìœ„: learning_rate (student_lr, teacher_lr)
```

#### 2. ë°ì´í„°ì…‹ë³„ ê¶Œì¥ê°’

**CIFAR-100:**
```yaml
ib_beta: 0.01
ce_alpha: 0.3
temperature: 4.0
mbm_out_dim: 512
student_lr: 0.001
teacher_lr: 0.0002
```

**ImageNet:**
```yaml
ib_beta: 0.05
ce_alpha: 0.4
temperature: 6.0
mbm_out_dim: 1024
student_lr: 0.0005
teacher_lr: 0.0001
```

**Continual Learning:**
```yaml
ib_beta: 0.1
ce_alpha: 0.3
temperature: 4.0
mbm_out_dim: 512
lambda_D: 1.0
lambda_IB: 1.0
```

#### 3. ì„±ëŠ¥ë³„ ìµœì í™” ì „ëµ

**ì •í™•ë„ ìµœì í™”:**
```yaml
ib_beta: 0.01  # ì•½í•œ ì••ì¶•ìœ¼ë¡œ ì •ë³´ ë³´ì¡´
ce_alpha: 0.3  # ê· í˜•ì¡íŒ í•™ìŠµ
temperature: 4.0  # ì ë‹¹í•œ ë¶€ë“œëŸ¬ì›€
```

**ì†ë„ ìµœì í™”:**
```yaml
use_amp: true  # Mixed Precision
batch_size: 256  # í° ë°°ì¹˜
mbm_out_dim: 256  # ì‘ì€ ì°¨ì›
```

**ë©”ëª¨ë¦¬ ìµœì í™”:**
```yaml
batch_size: 64  # ì‘ì€ ë°°ì¹˜
mbm_out_dim: 256  # ì‘ì€ ì°¨ì›
use_amp: true  # Mixed Precision
```

---

## ğŸ”§ êµ¬í˜„ ì„¸ë¶€ì‚¬í•­

### 1. ì†ì‹¤ í•¨ìˆ˜

#### ê¸°ë³¸ ì†ì‹¤ í•¨ìˆ˜
```python
def forward(self, x, y=None):
    # Teacher íŠ¹ì§• ì¶”ì¶œ
    t1_out = self.teacher1(x)
    t2_out = self.teacher2(x)
    
    # Student íŠ¹ì§• ì¶”ì¶œ
    feat_dict, s_logit, _ = self.student(x)
    s_feat = feat_dict["feat_2d"]
    
    # MBMì„ í†µí•œ ì‹œë„ˆì§€ íŠ¹ì§• ìƒì„±
    syn_feat, *_ = self.mbm(s_feat, feats_2d)
    
    # ì†ì‹¤ ê³„ì‚°
    ce_loss = self.ce_loss_fn(s_logit, y)
    kl_loss = F.kl_div(
        F.log_softmax(s_logit / self.T, dim=1),
        F.softmax(syn_logit / self.T, dim=1),
        reduction='batchmean'
    ) * (self.T ** 2)
    
    total_loss = self.alpha * ce_loss + (1 - self.alpha) * kl_loss
    return total_loss, s_logit
```

#### IB ì†ì‹¤ í•¨ìˆ˜ (ASIB-CL)
```python
def _compute_separated_losses(self, inputs, targets, features):
    # IB ì¸ì½”ë”
    mu, logvar = self._ib_encoder(features).chunk(2, dim=-1)
    z = self._reparameterize(mu, logvar)
    
    # IB ë””ì½”ë”
    reconstructed = self._ib_decoder(z)
    
    # IB ì†ì‹¤
    recon_loss = F.mse_loss(reconstructed, features)
    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())
    
    # ë¶„ë¦¬ëœ ì†ì‹¤
    ib_loss = recon_loss + self._ib_beta * kl_loss
    student_loss = self.lambda_D * ce_loss + self.lambda_IB * ib_loss
    
    return {
        'ib_loss': ib_loss,
        'student_loss': student_loss,
        'total_loss': student_loss
    }
```

### 2. Multi-Stage í•™ìŠµ

#### Stage A: Teacher Adaptive Update
```python
def _teacher_adaptive_update(self, train_loader, optimizer, epochs, stage=1):
    """Teacher ëª¨ë¸ë“¤ ì ì‘ì  ì—…ë°ì´íŠ¸"""
    for epoch in range(epochs):
        for inputs, targets in train_loader:
            # Teacher íŠ¹ì§• ì¶”ì¶œ
            t1_feat = self.teacher1(inputs)
            t2_feat = self.teacher2(inputs)
            
            # MBM ì—…ë°ì´íŠ¸
            syn_feat = self.mbm(t1_feat, t2_feat)
            
            # CCCP ì†ì‹¤ ê³„ì‚°
            if self.config.get("use_cccp", True):
                tau = self.config.get("tau", 4.0)
                loss = self._compute_cccp_loss(syn_feat, targets, tau)
            else:
                loss = self.ce_loss_fn(syn_feat, targets)
            
            # ì—­ì „íŒŒ
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
```

#### Stage B: Student Distillation
```python
def _student_distill_update(self, train_loader, test_loader, optimizer, scheduler, epochs, stage=1):
    """Student ì§€ì‹ ì¦ë¥˜"""
    for epoch in range(epochs):
        for inputs, targets in train_loader:
            # Forward pass
            total_loss, student_logit = self.forward(inputs, targets)
            
            # ì—­ì „íŒŒ
            optimizer.zero_grad()
            total_loss.backward()
            optimizer.step()
        
        # ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸
        if scheduler is not None:
            scheduler.step()
```

### 3. ì •ë³´ ë³‘ëª© ëª¨ë“ˆ

#### IB ì¸ì½”ë”/ë””ì½”ë”
```python
def _init_ib_modules(self, feature_dim):
    """Information Bottleneck ëª¨ë“ˆ ì´ˆê¸°í™”"""
    latent_dim = feature_dim // 4  # ì••ì¶•ëœ í‘œí˜„ ì°¨ì›
    
    self._ib_encoder = nn.Sequential(
        nn.Linear(feature_dim, feature_dim // 2),
        nn.ReLU(),
        nn.Linear(feature_dim // 2, latent_dim * 2)  # mu, logvar
    )
    
    self._ib_decoder = nn.Sequential(
        nn.Linear(latent_dim, feature_dim // 2),
        nn.ReLU(),
        nn.Linear(feature_dim // 2, feature_dim)
    )
```

---

## ğŸ§ª ì‹¤í—˜ ì„¤ì •

### ğŸ“‹ ì‹¤í—˜ í™˜ê²½ êµ¬ì„±

#### 1. í•˜ë“œì›¨ì–´ ìš”êµ¬ì‚¬í•­
```yaml
# ìµœì†Œ ìš”êµ¬ì‚¬í•­
GPU: NVIDIA RTX 3080 (12GB VRAM)
CPU: Intel i7-10700K ë˜ëŠ” AMD Ryzen 7 3700X
RAM: 32GB DDR4
Storage: 500GB SSD

# ê¶Œì¥ ì‚¬ì–‘
GPU: NVIDIA RTX 4090 (24GB VRAM) ë˜ëŠ” A100 (40GB VRAM)
CPU: Intel i9-12900K ë˜ëŠ” AMD Ryzen 9 5950X
RAM: 64GB DDR4
Storage: 1TB NVMe SSD
```

#### 2. ì†Œí”„íŠ¸ì›¨ì–´ í™˜ê²½
```bash
# Python í™˜ê²½
Python: 3.8-3.11
PyTorch: 2.0.0+
CUDA: 11.8+
cuDNN: 8.6+

# í•„ìˆ˜ íŒ¨í‚¤ì§€
torch>=2.0.0
torchvision>=0.15.0
numpy>=1.21.0
scipy>=1.7.0
tqdm>=4.62.0
matplotlib>=3.5.0
tensorboard>=2.8.0
hydra-core>=1.3.0
omegaconf>=2.2.0
```

### ğŸ¯ ì‹¤í—˜ ì‹œë‚˜ë¦¬ì˜¤ë³„ ì„¤ì •

#### 1. ASIB-KD (Knowledge Distillation) ì‹¤í—˜

##### 1.1 CIFAR-100 ì‹¤í—˜ ì„¤ì •
```yaml
# configs/experiment/asib_cifar100.yaml
defaults:
  - base
  - method: asib
  - dataset: cifar100
  - _self_

# ëª¨ë¸ ì„¤ì •
model:
  teacher1:
    name: convnext_l
    pretrained: true
    num_classes: 100
  teacher2:
    name: resnet152
    pretrained: true
    num_classes: 100
  student:
    name: resnet50_scratch
    pretrained: false
    num_classes: 100

# ASIB í•˜ì´í¼íŒŒë¼ë¯¸í„°
method:
  name: asib
  ce_alpha: 0.3
  kd_ens_alpha: 0.7
  use_ib: true
  ib_beta: 0.001
  ib_beta_warmup_epochs: 3
  use_cccp: true
  tau: 4.0

# MBM ì„¤ì •
mbm_query_dim: 1024
mbm_out_dim: 1024
mbm_n_head: 8
mbm_dropout: 0.0
mbm_learnable_q: false
mbm_reg_lambda: 0.0

# í•™ìŠµ ì„¤ì •
num_stages: 4
teacher_lr: 0.0002
student_lr: 0.001
teacher_weight_decay: 0.0001
student_weight_decay: 0.0003
student_epochs_per_stage: 15

# ë°ì´í„° ì„¤ì •
batch_size: 128
num_workers: 8
data_aug: true
mixup_alpha: 0.0
cutmix_alpha_distill: 0.3

# ìµœì í™” ì„¤ì •
use_amp: true
amp_dtype: float16
grad_clip_norm: 1.0
adam_beta1: 0.9
adam_beta2: 0.999

# ì •ê·œí™” ì„¤ì •
reg_lambda: 0.0
weight_decay: 0.0001

# ë””ë°”ì´ìŠ¤ ì„¤ì •
device: cuda
seed: 42
```

##### 1.2 ImageNet ì‹¤í—˜ ì„¤ì •
```yaml
# configs/experiment/asib_imagenet.yaml
defaults:
  - base
  - method: asib
  - dataset: imagenet
  - _self_

# ëª¨ë¸ ì„¤ì •
model:
  teacher1:
    name: convnext_l
    pretrained: true
    num_classes: 1000
  teacher2:
    name: efficientnet_l2
    pretrained: true
    num_classes: 1000
  student:
    name: resnet50_scratch
    pretrained: false
    num_classes: 1000

# ASIB í•˜ì´í¼íŒŒë¼ë¯¸í„° (ImageNetì— ìµœì í™”)
method:
  name: asib
  ce_alpha: 0.4
  kd_ens_alpha: 0.6
  use_ib: true
  ib_beta: 0.005
  ib_beta_warmup_epochs: 5
  use_cccp: true
  tau: 6.0

# MBM ì„¤ì • (ë” í° ëª¨ë¸)
mbm_query_dim: 2048
mbm_out_dim: 2048
mbm_n_head: 16
mbm_dropout: 0.1
mbm_learnable_q: true
mbm_reg_lambda: 0.001

# í•™ìŠµ ì„¤ì • (ImageNetì— ë§ì¶¤)
num_stages: 3
teacher_lr: 0.0001
student_lr: 0.0005
teacher_weight_decay: 0.0001
student_weight_decay: 0.0001
student_epochs_per_stage: 30

# ë°ì´í„° ì„¤ì •
batch_size: 256
num_workers: 16
data_aug: true
mixup_alpha: 0.2
cutmix_alpha_distill: 0.5

# ìµœì í™” ì„¤ì •
use_amp: true
amp_dtype: bfloat16  # ImageNetì—ì„œëŠ” bfloat16ì´ ë” ì•ˆì •ì 
grad_clip_norm: 1.0
adam_beta1: 0.9
adam_beta2: 0.999

# ì •ê·œí™” ì„¤ì •
reg_lambda: 0.001
weight_decay: 0.0001

# ë””ë°”ì´ìŠ¤ ì„¤ì •
device: cuda
seed: 42
```

#### 2. ASIB-CL (Continual Learning) ì‹¤í—˜

##### 2.1 CIFAR-100 Class-IL ì„¤ì •
```json
{
    "prefix": "asib_cl_cifar100",
    "dataset": "cifar100",
    "init_cls": 10,
    "increment": 10,
    "memory_size": 2000,
    "memory_per_class": 20,
    "fixed_memory": false,
    "shuffle": true,
    "convnet_type": "resnet32",
    "model_name": "asib_cl",
    "device": ["0"],
    "seed": [1993],
    
    "ib_beta": 0.1,
    "lambda_D": 1.0,
    "lambda_IB": 1.0,
    
    "batch_size": 64,
    "epochs": 200,
    "learning_rate": 0.1,
    "weight_decay": 5e-4,
    "milestones": [60, 120, 160],
    "gamma": 0.2,
    
    "num_workers": 8,
    "topk": 5,
    
    "logdir": "./experiments/sota/logs/asib_cl",
    "save_path": "./checkpoints/students/asib_cl"
}
```

##### 2.2 ImageNet-100 Class-IL ì„¤ì •
```json
{
    "prefix": "asib_cl_imagenet100",
    "dataset": "imagenet100",
    "init_cls": 10,
    "increment": 10,
    "memory_size": 5000,
    "memory_per_class": 50,
    "fixed_memory": false,
    "shuffle": true,
    "convnet_type": "resnet18",
    "model_name": "asib_cl",
    "device": ["0"],
    "seed": [1993],
    
    "ib_beta": 0.05,
    "lambda_D": 1.0,
    "lambda_IB": 1.0,
    
    "batch_size": 128,
    "epochs": 100,
    "learning_rate": 0.1,
    "weight_decay": 1e-4,
    "milestones": [30, 60, 80],
    "gamma": 0.1,
    
    "num_workers": 16,
    "topk": 5,
    
    "logdir": "./experiments/sota/logs/asib_cl_imagenet100",
    "save_path": "./checkpoints/students/asib_cl_imagenet100"
}
```

### ğŸ”¬ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì‹¤í—˜

#### 1. IB ë² íƒ€ (Î²) íŠœë‹ ì‹¤í—˜
```python
# IB ë² íƒ€ íŠœë‹ ìŠ¤í¬ë¦½íŠ¸
def ib_beta_tuning_experiment():
    """IB ë² íƒ€ ê°’ì— ë”°ë¥¸ ì„±ëŠ¥ ë¹„êµ ì‹¤í—˜"""
    
    beta_values = [0.001, 0.01, 0.05, 0.1, 0.2, 0.5, 1.0]
    results = {}
    
    for beta in beta_values:
        print(f"Testing ib_beta = {beta}")
        
        # ì„¤ì • íŒŒì¼ ì—…ë°ì´íŠ¸
        config = load_config('configs/experiment/asib_cifar100.yaml')
        config['method']['ib_beta'] = beta
        
        # ì‹¤í—˜ ì‹¤í–‰
        result = run_experiment(config)
        
        results[beta] = {
            'student_acc': result['student_accuracy'],
            'teacher_agreement': result['teacher_agreement'],
            'knowledge_transfer': result['knowledge_transfer_efficiency']
        }
    
    # ê²°ê³¼ ë¶„ì„ ë° ì‹œê°í™”
    plot_ib_beta_results(results)
    return results

# ê²°ê³¼ ì‹œê°í™”
def plot_ib_beta_results(results):
    """IB ë² íƒ€ íŠœë‹ ê²°ê³¼ ì‹œê°í™”"""
    import matplotlib.pyplot as plt
    
    betas = list(results.keys())
    student_accs = [results[beta]['student_acc'] for beta in betas]
    teacher_agreements = [results[beta]['teacher_agreement'] for beta in betas]
    
    plt.figure(figsize=(12, 5))
    
    plt.subplot(1, 2, 1)
    plt.semilogx(betas, student_accs, 'bo-', linewidth=2, markersize=8)
    plt.xlabel('IB Beta (Î²)')
    plt.ylabel('Student Accuracy (%)')
    plt.title('Student Accuracy vs IB Beta')
    plt.grid(True)
    
    plt.subplot(1, 2, 2)
    plt.semilogx(betas, teacher_agreements, 'ro-', linewidth=2, markersize=8)
    plt.xlabel('IB Beta (Î²)')
    plt.ylabel('Teacher Agreement (%)')
    plt.title('Teacher Agreement vs IB Beta')
    plt.grid(True)
    
    plt.tight_layout()
    plt.savefig('ib_beta_tuning_results.png', dpi=300, bbox_inches='tight')
    plt.show()
```

#### 2. MBM ì°¨ì› íŠœë‹ ì‹¤í—˜
```python
# MBM ì¶œë ¥ ì°¨ì› íŠœë‹
def mbm_dimension_tuning_experiment():
    """MBM ì¶œë ¥ ì°¨ì›ì— ë”°ë¥¸ ì„±ëŠ¥ ë¹„êµ ì‹¤í—˜"""
    
    mbm_dims = [256, 512, 1024, 2048, 4096]
    results = {}
    
    for dim in mbm_dims:
        print(f"Testing mbm_out_dim = {dim}")
        
        # ì„¤ì • íŒŒì¼ ì—…ë°ì´íŠ¸
        config = load_config('configs/experiment/asib_cifar100.yaml')
        config['mbm_out_dim'] = dim
        config['mbm_query_dim'] = dim
        
        # ì‹¤í—˜ ì‹¤í–‰
        result = run_experiment(config)
        
        results[dim] = {
            'student_acc': result['student_accuracy'],
            'memory_usage': result['memory_usage'],
            'training_time': result['training_time']
        }
    
    # ê²°ê³¼ ë¶„ì„
    plot_mbm_dimension_results(results)
    return results

def plot_mbm_dimension_results(results):
    """MBM ì°¨ì› íŠœë‹ ê²°ê³¼ ì‹œê°í™”"""
    import matplotlib.pyplot as plt
    
    dims = list(results.keys())
    student_accs = [results[dim]['student_acc'] for dim in dims]
    memory_usage = [results[dim]['memory_usage'] for dim in dims]
    training_time = [results[dim]['training_time'] for dim in dims]
    
    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 5))
    
    ax1.plot(dims, student_accs, 'bo-', linewidth=2, markersize=8)
    ax1.set_xlabel('MBM Output Dimension')
    ax1.set_ylabel('Student Accuracy (%)')
    ax1.set_title('Accuracy vs MBM Dimension')
    ax1.grid(True)
    
    ax2.plot(dims, memory_usage, 'ro-', linewidth=2, markersize=8)
    ax2.set_xlabel('MBM Output Dimension')
    ax2.set_ylabel('Memory Usage (GB)')
    ax2.set_title('Memory Usage vs MBM Dimension')
    ax2.grid(True)
    
    ax3.plot(dims, training_time, 'go-', linewidth=2, markersize=8)
    ax3.set_xlabel('MBM Output Dimension')
    ax3.set_ylabel('Training Time (hours)')
    ax3.set_title('Training Time vs MBM Dimension')
    ax3.grid(True)
    
    plt.tight_layout()
    plt.savefig('mbm_dimension_tuning_results.png', dpi=300, bbox_inches='tight')
    plt.show()
```

#### 3. í•™ìŠµë¥  íŠœë‹ ì‹¤í—˜
```python
# í•™ìŠµë¥  íŠœë‹
def learning_rate_tuning_experiment():
    """í•™ìŠµë¥  íŠœë‹ ì‹¤í—˜"""
    
    lr_combinations = [
        {'teacher_lr': 0.0001, 'student_lr': 0.0005},
        {'teacher_lr': 0.0002, 'student_lr': 0.001},
        {'teacher_lr': 0.0005, 'student_lr': 0.002},
        {'teacher_lr': 0.001, 'student_lr': 0.005},
        {'teacher_lr': 0.002, 'student_lr': 0.01}
    ]
    
    results = {}
    
    for i, lr_combo in enumerate(lr_combinations):
        print(f"Testing combination {i+1}: {lr_combo}")
        
        # ì„¤ì • íŒŒì¼ ì—…ë°ì´íŠ¸
        config = load_config('configs/experiment/asib_cifar100.yaml')
        config['teacher_lr'] = lr_combo['teacher_lr']
        config['student_lr'] = lr_combo['student_lr']
        
        # ì‹¤í—˜ ì‹¤í–‰
        result = run_experiment(config)
        
        results[f"combo_{i+1}"] = {
            'teacher_lr': lr_combo['teacher_lr'],
            'student_lr': lr_combo['student_lr'],
            'student_acc': result['student_accuracy'],
            'convergence_epoch': result['convergence_epoch']
        }
    
    return results
```

### ğŸ“Š ì‹¤í—˜ ê²°ê³¼ ë¶„ì„

#### 1. ì„±ëŠ¥ ì§€í‘œ ì •ì˜
```python
# ì„±ëŠ¥ ì§€í‘œ ê³„ì‚° í•¨ìˆ˜ë“¤
def calculate_student_accuracy(model, test_loader):
    """í•™ìƒ ëª¨ë¸ ì •í™•ë„ ê³„ì‚°"""
    model.eval()
    correct = 0
    total = 0
    
    with torch.no_grad():
        for inputs, targets in test_loader:
            inputs, targets = inputs.to(model.device), targets.to(model.device)
            outputs = model(inputs)
            _, predicted = outputs.max(1)
            total += targets.size(0)
            correct += predicted.eq(targets).sum().item()
    
    return 100. * correct / total

def calculate_teacher_agreement(teacher1, teacher2, test_loader):
    """êµì‚¬ ëª¨ë¸ë“¤ ê°„ì˜ ì¼ì¹˜ë„ ê³„ì‚°"""
    teacher1.eval()
    teacher2.eval()
    agreement = 0
    total = 0
    
    with torch.no_grad():
        for inputs, _ in test_loader:
            inputs = inputs.to(teacher1.device)
            outputs1 = teacher1(inputs)
            outputs2 = teacher2(inputs)
            
            _, pred1 = outputs1.max(1)
            _, pred2 = outputs2.max(1)
            
            agreement += (pred1 == pred2).sum().item()
            total += inputs.size(0)
    
    return 100. * agreement / total

def calculate_knowledge_transfer_efficiency(student, teacher1, teacher2, test_loader):
    """ì§€ì‹ ì „ë‹¬ íš¨ìœ¨ì„± ê³„ì‚°"""
    # KL Divergence ê¸°ë°˜ íš¨ìœ¨ì„± ê³„ì‚°
    student.eval()
    teacher1.eval()
    teacher2.eval()
    
    total_kl = 0
    total_samples = 0
    
    with torch.no_grad():
        for inputs, _ in test_loader:
            inputs = inputs.to(student.device)
            
            # í™•ë¥  ë¶„í¬ ê³„ì‚°
            student_probs = F.softmax(student(inputs), dim=1)
            teacher1_probs = F.softmax(teacher1(inputs), dim=1)
            teacher2_probs = F.softmax(teacher2(inputs), dim=1)
            
            # ì•™ìƒë¸” êµì‚¬ í™•ë¥ 
            ensemble_probs = (teacher1_probs + teacher2_probs) / 2
            
            # KL Divergence ê³„ì‚°
            kl_div = F.kl_div(
                student_probs.log(), ensemble_probs, reduction='batchmean'
            )
            
            total_kl += kl_div.item()
            total_samples += 1
    
    avg_kl = total_kl / total_samples
    # íš¨ìœ¨ì„±ì€ KL Divergenceì˜ ì—­ìˆ˜ (ë‚®ì„ìˆ˜ë¡ íš¨ìœ¨ì )
    efficiency = 1.0 / (1.0 + avg_kl)
    
    return efficiency
```

#### 2. ì‹¤í—˜ ê²°ê³¼ ì‹œê°í™”
```python
# ì‹¤í—˜ ê²°ê³¼ ì‹œê°í™”
def visualize_experiment_results(results, save_path='experiment_results.png'):
    """ì‹¤í—˜ ê²°ê³¼ ì¢…í•© ì‹œê°í™”"""
    import matplotlib.pyplot as plt
    import seaborn as sns
    
    # ìŠ¤íƒ€ì¼ ì„¤ì •
    plt.style.use('seaborn-v0_8')
    sns.set_palette("husl")
    
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    
    # 1. ì •í™•ë„ ë¹„êµ
    methods = list(results.keys())
    accuracies = [results[method]['student_acc'] for method in methods]
    
    axes[0, 0].bar(methods, accuracies, color='skyblue', alpha=0.7)
    axes[0, 0].set_title('Student Accuracy Comparison')
    axes[0, 0].set_ylabel('Accuracy (%)')
    axes[0, 0].tick_params(axis='x', rotation=45)
    
    # 2. í•™ìŠµ ê³¡ì„ 
    for method in methods:
        train_loss = results[method]['train_loss']
        val_loss = results[method]['val_loss']
        epochs = range(1, len(train_loss) + 1)
        
        axes[0, 1].plot(epochs, train_loss, label=f'{method} (Train)', alpha=0.7)
        axes[0, 1].plot(epochs, val_loss, label=f'{method} (Val)', linestyle='--', alpha=0.7)
    
    axes[0, 1].set_title('Training Curves')
    axes[0, 1].set_xlabel('Epoch')
    axes[0, 1].set_ylabel('Loss')
    axes[0, 1].legend()
    axes[0, 1].grid(True)
    
    # 3. ì§€ì‹ ì „ë‹¬ íš¨ìœ¨ì„±
    efficiencies = [results[method]['knowledge_transfer'] for method in methods]
    
    axes[0, 2].bar(methods, efficiencies, color='lightgreen', alpha=0.7)
    axes[0, 2].set_title('Knowledge Transfer Efficiency')
    axes[0, 2].set_ylabel('Efficiency')
    axes[0, 2].tick_params(axis='x', rotation=45)
    
    # 4. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
    memory_usage = [results[method]['memory_usage'] for method in methods]
    
    axes[1, 0].bar(methods, memory_usage, color='orange', alpha=0.7)
    axes[1, 0].set_title('Memory Usage')
    axes[1, 0].set_ylabel('Memory (GB)')
    axes[1, 0].tick_params(axis='x', rotation=45)
    
    # 5. í•™ìŠµ ì‹œê°„
    training_time = [results[method]['training_time'] for method in methods]
    
    axes[1, 1].bar(methods, training_time, color='red', alpha=0.7)
    axes[1, 1].set_title('Training Time')
    axes[1, 1].set_ylabel('Time (hours)')
    axes[1, 1].tick_params(axis='x', rotation=45)
    
    # 6. í•˜ì´í¼íŒŒë¼ë¯¸í„° ë¯¼ê°ë„
    param_sensitivity = results.get('param_sensitivity', {})
    if param_sensitivity:
        params = list(param_sensitivity.keys())
        sensitivities = list(param_sensitivity.values())
        
        axes[1, 2].bar(params, sensitivities, color='purple', alpha=0.7)
        axes[1, 2].set_title('Parameter Sensitivity')
        axes[1, 2].set_ylabel('Sensitivity Score')
        axes[1, 2].tick_params(axis='x', rotation=45)
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.show()
```

### ğŸš€ ì‹¤í—˜ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸

#### 1. ë‹¨ì¼ ì‹¤í—˜ ì‹¤í–‰
```bash
#!/bin/bash
# run_single_experiment.sh

# í™˜ê²½ ì„¤ì •
export CUDA_VISIBLE_DEVICES=0
export PYTHONPATH="${PYTHONPATH}:$(pwd)"

# ì‹¤í—˜ ì„¤ì •
EXPERIMENT_NAME="asib_cifar100_baseline"
CONFIG_PATH="configs/experiment/asib_cifar100.yaml"
LOG_DIR="experiments/logs/${EXPERIMENT_NAME}"
SAVE_DIR="experiments/results/${EXPERIMENT_NAME}"

# ë””ë ‰í† ë¦¬ ìƒì„±
mkdir -p ${LOG_DIR}
mkdir -p ${SAVE_DIR}

# ì‹¤í—˜ ì‹¤í–‰
python main.py \
    --config-name=${CONFIG_PATH} \
    hydra.run.dir=${SAVE_DIR} \
    hydra.sweep.dir=${SAVE_DIR} \
    hydra.sweep.subdir=${EXPERIMENT_NAME} \
    2>&1 | tee ${LOG_DIR}/experiment.log

echo "Experiment completed: ${EXPERIMENT_NAME}"
```

#### 2. í•˜ì´í¼íŒŒë¼ë¯¸í„° ìŠ¤ìœ• ì‹¤í—˜
```bash
#!/bin/bash
# run_hyperparameter_sweep.sh

# í™˜ê²½ ì„¤ì •
export CUDA_VISIBLE_DEVICES=0,1,2,3
export PYTHONPATH="${PYTHONPATH}:$(pwd)"

# ìŠ¤ìœ• ì„¤ì •
SWEEP_NAME="asib_ib_beta_sweep"
BASE_CONFIG="configs/experiment/asib_cifar100.yaml"
LOG_DIR="experiments/logs/${SWEEP_NAME}"
SAVE_DIR="experiments/results/${SWEEP_NAME}"

# ë””ë ‰í† ë¦¬ ìƒì„±
mkdir -p ${LOG_DIR}
mkdir -p ${SAVE_DIR}

# Hydra ë©€í‹°ëŸ°ìœ¼ë¡œ ìŠ¤ìœ• ì‹¤í–‰
python main.py \
    --multirun \
    --config-name=${BASE_CONFIG} \
    method.ib_beta=0.001,0.01,0.05,0.1,0.2,0.5 \
    hydra.sweep.dir=${SAVE_DIR} \
    hydra.sweep.subdir=${SWEEP_NAME} \
    2>&1 | tee ${LOG_DIR}/sweep.log

echo "Hyperparameter sweep completed: ${SWEEP_NAME}"
```

#### 3. ë¹„êµ ì‹¤í—˜ ì‹¤í–‰
```bash
#!/bin/bash
# run_comparison_experiments.sh

# í™˜ê²½ ì„¤ì •
export CUDA_VISIBLE_DEVICES=0,1,2,3
export PYTHONPATH="${PYTHONPATH}:$(pwd)"

# ë¹„êµí•  ë°©ë²•ë“¤
METHODS=("asib" "vanilla_kd" "fitnet" "attention" "hint" "crd")

# ê° ë°©ë²•ë³„ ì‹¤í—˜ ì‹¤í–‰
for method in "${METHODS[@]}"; do
    echo "Running experiment for method: ${method}"
    
    EXPERIMENT_NAME="comparison_${method}"
    CONFIG_PATH="configs/experiment/${method}_cifar100.yaml"
    LOG_DIR="experiments/logs/${EXPERIMENT_NAME}"
    SAVE_DIR="experiments/results/${EXPERIMENT_NAME}"
    
    # ë””ë ‰í† ë¦¬ ìƒì„±
    mkdir -p ${LOG_DIR}
    mkdir -p ${SAVE_DIR}
    
    # ì‹¤í—˜ ì‹¤í–‰
    python main.py \
        --config-name=${CONFIG_PATH} \
        hydra.run.dir=${SAVE_DIR} \
        hydra.sweep.dir=${SAVE_DIR} \
        hydra.sweep.subdir=${EXPERIMENT_NAME} \
        2>&1 | tee ${LOG_DIR}/experiment.log
    
    echo "Completed experiment for method: ${method}"
done

echo "All comparison experiments completed"
```

---

## ğŸš€ ì‚¬ìš©ë²•

### 1. ê¸°ë³¸ ASIB-KD ì‹¤í–‰

#### ì„¤ì • íŒŒì¼ ìƒì„±
```yaml
# configs/experiment/asib_experiment.yaml
defaults:
  - base
  - _self_

# ëª¨ë¸ ì„¤ì •
model:
  teacher1:
    name: convnext_l
    pretrained: true
  teacher2:
    name: resnet152
    pretrained: true
  student:
    name: resnet50_scratch
    pretrained: false

# ASIB ì„¤ì •
method:
  name: asib
  ce_alpha: 0.3
  kd_ens_alpha: 0.7
  use_ib: true
  ib_beta: 0.001
  use_cccp: true
  tau: 4.0

# í•™ìŠµ ì„¤ì •
num_stages: 4
teacher_lr: 0.0002
student_lr: 0.001
student_epochs_per_stage: 15
```

#### ì‹¤í–‰ ëª…ë ¹
```bash
# Hydraë¥¼ ì‚¬ìš©í•œ ì‹¤í–‰
python main.py --config-name=asib_experiment

# ì§ì ‘ ì‹¤í–‰
python main.py \
  --method=asib \
  --teacher1=convnext_l \
  --teacher2=resnet152 \
  --student=resnet50_scratch \
  --ib_beta=0.001 \
  --num_stages=4
```

### 2. ASIB-CL ì‹¤í–‰

#### ì„¤ì • íŒŒì¼
```json
{
    "convnet_type": "resnet32",
    "dataset": "cifar100",
    "init_cls": 10,
    "increment": 10,
    "memory_size": 2000,
    "memory_per_class": 20,
    "device": [0],
    "num_workers": 8,
    "batch_size": 128,
    "epochs": 170,
    "lr": 0.1,
    "lr_decay": 0.1,
    "milestones": [60, 120, 160],
    "weight_decay": 0.0002,
    "ib_beta": 0.1,
    "topk": 5,
    "seed": 1993,
    "logdir": "./experiments/sota/logs/asib_cl",
    "model_name": "asib_cl"
}
```

#### ì‹¤í–‰ ëª…ë ¹
```bash
# ASIB-CL ì‹¤í—˜ ì‹¤í–‰
python PyCIL/main.py --config=PyCIL/exps/asib_cl.json

# ë¹„êµ ì‹¤í—˜ ì‹¤í–‰
python run_asib_cl_experiment.py
```

### 3. í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹

#### ìë™ íŠœë‹ ìŠ¤í¬ë¦½íŠ¸
```python
def hyperparameter_tuning():
    """í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹"""
    
    # íŠœë‹í•  íŒŒë¼ë¯¸í„° ì¡°í•©
    param_combinations = [
        {'ib_beta': 0.01, 'lambda_D': 1.0, 'lambda_IB': 1.0},
        {'ib_beta': 0.1, 'lambda_D': 1.0, 'lambda_IB': 1.0},
        {'ib_beta': 0.5, 'lambda_D': 1.0, 'lambda_IB': 1.0},
        {'ib_beta': 1.0, 'lambda_D': 1.0, 'lambda_IB': 1.0},
        {'ib_beta': 0.1, 'lambda_D': 0.5, 'lambda_IB': 1.0},
        {'ib_beta': 0.1, 'lambda_D': 2.0, 'lambda_IB': 1.0},
        {'ib_beta': 0.1, 'lambda_D': 1.0, 'lambda_IB': 0.5},
        {'ib_beta': 0.1, 'lambda_D': 1.0, 'lambda_IB': 2.0},
    ]
    
    results = {}
    
    for i, params in enumerate(param_combinations):
        print(f"Testing combination {i+1}/{len(param_combinations)}: {params}")
        
        # ì„¤ì • íŒŒì¼ ì—…ë°ì´íŠ¸
        config = load_config('PyCIL/exps/asib_cl.json')
        config.update(params)
        save_config(config, f'PyCIL/exps/asib_cl_tune_{i}.json')
        
        # ì‹¤í—˜ ì‹¤í–‰
        result = run_experiment(f'PyCIL/exps/asib_cl_tune_{i}.json')
        results[f'combo_{i}'] = {
            'params': params,
            'aia': result['aia'],
            'af': result['af']
        }
    
    # ê²°ê³¼ ë¶„ì„
    analyze_tuning_results(results)
```

---

## âš¡ ì„±ëŠ¥ ìµœì í™”

### 1. í•™ìŠµ ì†ë„ ìµœì í™”

#### Mixed Precision Training
```python
from torch.cuda.amp import autocast, GradScaler

def _train_with_amp(self, train_loader, optimizer):
    """Mixed Precision Training"""
    scaler = GradScaler()
    
    for inputs, targets in train_loader:
        inputs, targets = inputs.to(self.device), targets.to(self.device)
        
        # Mixed precision forward pass
        with autocast():
            features = self._network.extract_vector(inputs)
            losses = self._compute_separated_losses(inputs, targets, features)
        
        # Scaled backward pass
        scaler.scale(losses['total_loss']).backward()
        scaler.step(optimizer)
        scaler.update()
        optimizer.zero_grad()
```

#### ë°ì´í„° ë¡œë”© ìµœì í™”
```python
# DataLoader ìµœì í™”
train_loader = DataLoader(
    train_dataset,
    batch_size=batch_size,
    shuffle=True,
    num_workers=4,  # ë©€í‹°í”„ë¡œì„¸ì‹±
    pin_memory=True,  # GPU ë©”ëª¨ë¦¬ í•€
    persistent_workers=True  # ì›Œì»¤ ì¬ì‚¬ìš©
)
```

### 2. ë©”ëª¨ë¦¬ ìµœì í™”

#### ê·¸ë˜ë””ì–¸íŠ¸ ì²´í¬í¬ì¸íŒ…
```python
# ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ í•™ìŠµ
from torch.utils.checkpoint import checkpoint

def forward_with_checkpoint(self, x):
    return checkpoint(self._forward_pass, x)
```

#### ëª¨ë¸ ì••ì¶•
```python
# ëª¨ë¸ ì–‘ìí™”
from torch.quantization import quantize_dynamic

def quantize_model(self, model):
    return quantize_dynamic(
        model, 
        {nn.Linear, nn.Conv2d}, 
        dtype=torch.qint8
    )
```

### 3. í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”

#### ë² ì´ì§€ì•ˆ ìµœì í™”
```python
from skopt import gp_minimize
from skopt.space import Real

def bayesian_optimization():
    """ë² ì´ì§€ì•ˆ ìµœì í™”ë¥¼ í†µí•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹"""
    
    # íƒìƒ‰ ê³µê°„ ì •ì˜
    space = [
        Real(0.01, 1.0, name='ib_beta'),
        Real(0.1, 10.0, name='lambda_D'),
        Real(0.1, 10.0, name='lambda_IB')
    ]
    
    def objective(params):
        ib_beta, lambda_D, lambda_IB = params
        
        # ì„¤ì • ì—…ë°ì´íŠ¸
        config = load_config('PyCIL/exps/asib_cl.json')
        config.update({
            'ib_beta': ib_beta,
            'lambda_D': lambda_D,
            'lambda_IB': lambda_IB
        })
        
        # ì‹¤í—˜ ì‹¤í–‰
        result = run_experiment(config)
        
        # ëª©í‘œ: AIA ìµœëŒ€í™”, AF ìµœì†Œí™”
        objective_value = result['af'] - result['aia']  # ìµœì†Œí™” ë¬¸ì œ
        return objective_value
    
    # ìµœì í™” ì‹¤í–‰
    result = gp_minimize(
        objective,
        space,
        n_calls=50,
        random_state=42
    )
    
    print(f"Best parameters: {result.x}")
    print(f"Best objective value: {result.fun}")
```
