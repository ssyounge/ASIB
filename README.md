# ASIB Knowledge Distillation Framework

**ASIB** (Adaptive Synergy Information-Bottleneck) is a multi-stage knowledge distillation framework that uses Information-Bottleneck Manifold Bridging Module (IB-MBM) to create synergistic knowledge from multiple teachers.

## ğŸš€ Quick Start

### Installation

```bash
# Clone repository
git clone https://github.com/YourName/ASIB-KD.git
cd ASIB-KD

# Create conda environment
conda env create -f environment.yml
conda activate asib

# Install dependencies
pip install -r requirements.txt
```

### Basic Usage

```bash
# Run default experiment
sbatch run/run.sh

# Run with custom config
python main.py experiment=res152_convnext_effi

# Run sensitivity analysis
sbatch run/run_sensitivity.sh

# Run overlap analysis
sbatch run/run_overlap.sh

# Run teacher fine-tuning
sbatch run/run_finetune_clean.sh
```

## ğŸ“ Project Structure

```
ASIB-KD/
â”œâ”€â”€ main.py                 # Main training script (simplified)
â”œâ”€â”€ eval.py                 # Model evaluation
â”œâ”€â”€ core/                   # Core functionality
â”‚   â”œâ”€â”€ builder.py         # Model creation utilities
â”‚   â”œâ”€â”€ trainer.py         # Training logic
â”‚   â””â”€â”€ utils.py           # Configuration utilities
â”œâ”€â”€ configs/               # Configuration files
â”‚   â”œâ”€â”€ base.yaml         # Base configuration
â”‚   â”œâ”€â”€ method/asib.yaml  # ASIB method config
â”‚   â”œâ”€â”€ experiment/       # Experiment configs
â”‚   â””â”€â”€ model/           # Model configs
â”œâ”€â”€ utils/                # Utilities (reorganized)
â”‚   â”œâ”€â”€ logging/         # Logging utilities
â”‚   â”œâ”€â”€ data/           # Data utilities
â”‚   â”œâ”€â”€ training/       # Training utilities
â”‚   â””â”€â”€ common/         # Common utilities
â”œâ”€â”€ modules/             # Training modules
â”œâ”€â”€ methods/            # Distillation methods
â”œâ”€â”€ models/             # Model definitions
â”œâ”€â”€ scripts/            # Utility scripts (organized)
â”‚   â”œâ”€â”€ analysis/       # Analysis scripts
â”‚   â”œâ”€â”€ training/       # Training scripts
â”‚   â””â”€â”€ setup/          # Setup scripts
â””â”€â”€ run/                # SLURM execution scripts
```

## ğŸ”§ Key Features

### ğŸ¯ **ASIB Method**
- **Multi-Stage Distillation**: Teacher â†” Student updates in phases
- **Information-Bottleneck MBM**: Fuses teacher features using IB principles
- **Adaptive Synergy**: Creates synergistic knowledge from multiple teachers

### ğŸ§Š **Partial Freezing**
- **Efficient Training**: Freeze backbone, adapt BN/Heads/MBM
- **Flexible Levels**: -1 (no freeze) to N (freeze N blocks)
- **Auto-Scheduling**: Stage-wise freeze level progression

### ğŸ¨ **Multiple KD Methods**
- **ASIB**: Our proposed method (default)
- **Vanilla KD**: Traditional knowledge distillation
- **FitNet**: Feature-level distillation
- **CRD**: Contrastive representation distillation
- **AT**: Attention transfer
- **DKD**: Decoupled knowledge distillation

### ğŸ“Š **Supported Datasets**
- **CIFAR-100**: 100-class image classification
- **ImageNet-32**: Downsampled ImageNet
- **Custom**: Easy to extend

## ğŸ› ï¸ Configuration

### Basic Configuration

```yaml
# configs/base.yaml
method: asib
num_stages: 4
teacher_lr: 0.0002
student_lr: 0.001
use_ib: true
ib_beta: 0.001
use_cccp: true  # Concave-Convex Procedure
```

### Model Configuration

```yaml
# configs/experiment/res152_convnext_effi.yaml
teacher1:
  model:
    teacher:
      name: convnext_l_teacher
teacher2:
  model:
    teacher:
      name: efficientnet_l2_teacher
model:
  student:
    model:
      student:
        name: resnet152_pretrain_student
```

### Training Configuration

```yaml
# Partial freezing
use_partial_freeze: true
student_freeze_schedule: [-1, 2, 1, 0]  # Stage-wise freeze levels

# MBM settings
mbm_query_dim: 1024
mbm_out_dim: 1024
mbm_n_head: 8

# Loss weights
ce_alpha: 0.3
kd_alpha: 0.7
ib_beta: 0.001
```

## ğŸš€ Usage Examples

### 1. Standard Training

```bash
# Run with default settings
python main.py

# Run with custom experiment
python main.py --config-name experiment/res152_convnext_effi

# Override parameters
python main.py student_lr=0.0005 ib_beta=0.01
```

### 2. Teacher Fine-tuning

```bash
# Fine-tune teachers before distillation
python scripts/fine_tuning.py --config-name base \
  +teacher_type=resnet152 +finetune_epochs=100
```

### 3. Student Baseline

```bash
# Train student alone for baseline
python scripts/train_student_baseline.py --config-name base
```

### 4. Evaluation

```bash
# Evaluate single model
python eval.py +eval_mode=single +ckpt_path=./results/student_final.pth

# Evaluate synergy model
python eval.py +eval_mode=synergy
```

## ğŸ”¬ Advanced Features

### CCCP (Concave-Convex Procedure)

Enable CCCP for stable teacher updates:

```yaml
use_cccp: true
tau: 4.0  # Temperature for CCCP
```

### Continual Learning

```bash
# Enable continual learning mode
python main.py cl_mode=true num_tasks=5
```

### Data Augmentation

```bash
# Enable/disable augmentation
python main.py data_aug=true
python main.py data_aug=false

# MixUp and CutMix
python main.py mixup_alpha=0.2 cutmix_alpha_distill=0.3
```

### Automatic Mixed Precision

```yaml
use_amp: true
amp_dtype: float16  # or bfloat16
```

## ğŸ“ˆ Results

Results are saved in the `outputs/` directory:

```
outputs/
â”œâ”€â”€ experiment_name/
â”‚   â”œâ”€â”€ train.log          # Training logs
â”‚   â”œâ”€â”€ metrics.csv        # Performance metrics
â”‚   â”œâ”€â”€ config.yaml        # Final configuration
â”‚   â””â”€â”€ checkpoints/       # Model checkpoints
```

## ğŸ§ª Testing

```bash
# Run all tests
pytest

# Run specific test
pytest tests/test_asib_step.py
```

## ğŸ“š API Reference

### Core Functions

```python
from core import (
    create_student_by_name,
    create_teacher_by_name,
    run_training_stages,
    run_continual_learning
)

# Create models
student = create_student_by_name("resnet152_pretrain_student")
teacher1 = create_teacher_by_name("convnext_l_teacher")

# Run training
final_acc = run_training_stages(
    teacher_wrappers=[teacher1, teacher2],
    mbm=mbm,
    synergy_head=synergy_head,
    student_model=student,
    train_loader=train_loader,
    test_loader=test_loader,
    cfg=cfg,
    exp_logger=exp_logger,
    num_stages=4
)
```

### Configuration Utilities

```python
from core.utils import (
    setup_partial_freeze_schedule,
    auto_set_mbm_query_dim,
    cast_numeric_configs
)

# Setup training configuration
setup_partial_freeze_schedule(cfg, num_stages)
auto_set_mbm_query_dim(student_model, cfg)
cast_numeric_configs(cfg)
```

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests if applicable
5. Submit a pull request

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ“– Citation

If you use this framework in your research, please cite:

```bibtex
@misc{asib-kd,
  title={ASIB: Adaptive Synergy Information-Bottleneck Knowledge Distillation},
  author={Suyoung Yang},
  year={2024},
  howpublished={\url{https://github.com/YourName/ASIB-KD}}
}
```

## ğŸ“ Contact

- **Email**: suyoung425@yonsei.ac.kr
- **GitHub Issues**: [Create an issue](https://github.com/YourName/ASIB-KD/issues)

---

## ğŸ”„ Recent Updates

### v2.0.0 (Latest)
- âœ… **Code Refactoring**: Modular structure with `core/` package
- âœ… **Utils Reorganization**: Functional subfolders in `utils/`
- âœ… **ASMB â†’ ASIB**: Renamed to reflect Information-Bottleneck approach
- âœ… **CCCP Integration**: Added Concave-Convex Procedure for stability
- âœ… **Main.py Simplification**: Reduced from 955 to 329 lines (66% reduction)

### v1.0.0
- ğŸ¯ Initial release with ASMB framework
- ğŸ§Š Partial freezing mechanism
- ğŸ“Š Multi-teacher distillation
- ğŸ¨ Multiple KD methods support
